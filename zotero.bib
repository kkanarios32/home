@article{agarwalReinforcementLearningTheory,
  title = {Reinforcement {{Learning}}: {{Theory}} and {{Algorithms}}},
  author = {Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},
  langid = {english},
  file = {/home/kellen/Zotero/storage/Z3QU86C2/Agarwal et al. - Reinforcement
          Learning Theory and Algorithms.pdf},
}

@misc{carvalhoPredictiveRepresentationsBuilding2024,
  title = {Predictive Representations: Building Blocks of Intelligence},
  shorttitle = {Predictive Representations},
  author = {Carvalho, Wilka and Tomov, Momchil S. and {de Cothi}, William and
            Barry, Caswell and Gershman, Samuel J.},
  year = {2024},
  month = jul,
  number = {arXiv:2402.06590},
  eprint = {2402.06590},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-11},
  abstract = {Adaptive behavior often requires predicting future events. The
              theory of reinforcement learning prescribes what kinds of
              predictive representations are useful and how to compute them. This
              paper integrates these theoretical ideas with work on cognition and
              neuroscience. We pay special attention to the successor
              representation (SR) and its generalizations, which have been widely
              applied both as engineering tools and models of brain function.
              This convergence suggests that particular kinds of predictive
              representations may function as versatile building blocks of
              intelligence.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning},
  file = {/home/kellen/Zotero/storage/AWBJ53L5/Carvalho et al. - 2024 -
          Predictive representations building blocks of intelligence.pdf},
}

@misc{charikarQuantifyingGainWeakStrong2024,
  title = {Quantifying the {{Gain}} in {{Weak-to-Strong Generalization}}},
  author = {Charikar, Moses and Pabbaraju, Chirag and Shiragur, Kirankumar},
  year = {2024},
  month = oct,
  number = {arXiv:2405.15116},
  eprint = {2405.15116},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-24},
  abstract = {Recent advances in large language models have shown capabilities
              that are extraordinary and near-superhuman. These models operate
              with such complexity that reliably evaluating and aligning them
              proves challenging for humans. This leads to the natural question:
              can guidance from weak models (like humans) adequately direct the
              capabilities of strong models? In a recent and somewhat surprising
              work, Burns et al. [BIK+23] empirically demonstrated that when
              strong models (like GPT-4) are finetuned using labels generated by
              weak supervisors (like GPT-2), the strong models outperform their
              weaker counterparts---a phenomenon they term weak-to-strong
              generalization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning},
  file = {/home/kellen/Zotero/storage/Y3LYJ8Q8/Charikar et al. - 2024 -
          Quantifying the Gain in Weak-to-Strong Generalization.pdf},
}

@misc{chenDeepZeroScalingZerothOrder2024,
  title = {{{DeepZero}}: {{Scaling}} up {{Zeroth-Order Optimization}} for {{Deep
           Model Training}}},
  shorttitle = {{{DeepZero}}},
  author = {Chen, Aochuan and Zhang, Yimeng and Jia, Jinghan and Diffenderfer,
            James and Liu, Jiancheng and Parasyris, Konstantinos and Zhang, Yihua
            and Zhang, Zheng and Kailkhura, Bhavya and Liu, Sijia},
  year = {2024},
  month = mar,
  number = {arXiv:2310.02025},
  eprint = {2310.02025},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.02025},
  urldate = {2024-09-08},
  abstract = {Zeroth-order (ZO) optimization has become a popular technique for
              solving machine learning (ML) problems when first-order (FO)
              information is difficult or impossible to obtain. However, the
              scalability of ZO optimization remains an open problem: Its use has
              primarily been limited to relatively small-scale ML problems, such
              as sample-wise adversarial attack generation. To our best knowledge
              , no prior work has demonstrated the effectiveness of ZO
              optimization in training deep neural networks (DNNs) without a
              significant decrease in performance. To overcome this roadblock, we
              develop DeepZero, a principled ZO deep learning (DL) framework that
              can scale ZO optimization to DNN training from scratch through
              three primary innovations. First, we demonstrate the advantages of
              coordinatewise gradient estimation (CGE) over randomized
              vector-wise gradient estimation in training accuracy and
              computational efficiency. Second, we propose a sparsityinduced ZO
              training protocol that extends the model pruning methodology using
              only finite differences to explore and exploit the sparse DL prior
              in CGE. Third, we develop the methods of feature reuse and forward
              parallelization to advance the practical implementations of ZO
              training. Our extensive experiments show that DeepZero achieves
              state-of-the-art (SOTA) accuracy on ResNet-20 trained on CIFAR-10,
              approaching FO training performance for the first time. Furthermore
              , we show the practical utility of DeepZero in applications of
              certified adversarial defense and DL-based partial differential
              equation error correction, achieving 10-20\% improvement over SOTA.
              We believe our results will inspire future research on scalable ZO
              optimization and contribute to advancing DL with black box. Codes
              are available at https://github.com/OPTML-Group/DeepZero.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/kellen/Zotero/storage/7ALRQ8GJ/Chen et al. - 2024 - DeepZero
          Scaling up Zeroth-Order Optimization for Deep Model Training.pdf},
}

@misc{choMiniBatchOptimizationContrastive2023,
  title = {Mini-{{Batch Optimization}} of {{Contrastive Loss}}},
  author = {Cho, Jaewoong and Sreenivasan, Kartik and Lee, Keon and Mun,
            Kyunghoo and Yi, Soheun and Lee, Jeong-Gwan and Lee, Anna and Sohn,
            Jy-yong and Papailiopoulos, Dimitris and Lee, Kangwook},
  year = {2023},
  month = jul,
  journal = {arXiv.org},
  urldate = {2024-09-06},
  abstract = {Contrastive learning has gained significant attention as a method
              for self-supervised learning. The contrastive loss function ensures
              that embeddings of positive sample pairs (e.g., different samples
              from the same class or different views of the same object) are
              similar, while embeddings of negative pairs are dissimilar.
              Practical constraints such as large memory requirements make it
              challenging to consider all possible positive and negative pairs,
              leading to the use of mini-batch optimization. In this paper, we
              investigate the theoretical aspects of mini-batch optimization in
              contrastive learning. We show that mini-batch optimization is
              equivalent to full-batch optimization if and only if all \${
              \textbackslash}binom\{N\}\{B\}\$ mini-batches are selected, while
              sub-optimality may arise when examining only a subset. We then
              demonstrate that utilizing high-loss mini-batches can speed up SGD
              convergence and propose a spectral clustering-based approach for
              identifying these high-loss mini-batches. Our experimental results
              validate our theoretical findings and demonstrate that our proposed
              algorithm outperforms vanilla SGD in practically relevant settings,
              providing a better understanding of mini-batch optimization in
              contrastive learning.},
  howpublished = {https://arxiv.org/abs/2307.05906v1},
  langid = {english},
  file = {/home/kellen/Zotero/storage/G87QGI8T/Cho et al. - 2023 - Mini-Batch
          Optimization of Contrastive Loss.pdf},
}

@book{cinlarProbabilityStochastics2011,
  title = {Probability and {{Stochastics}}},
  author = {{\c C}inlar, Erhan},
  year = {2011},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {261},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-0-387-87859-1},
  urldate = {2024-10-18},
  copyright = {
               https://www.springernature.com/gp/researchers/text-and-data-mining
               },
  isbn = {978-0-387-87858-4 978-0-387-87859-1},
  langid = {english},
  file = {/home/kellen/Zotero/storage/GHJPXQJR/Çinlar - 2011 - Probability and
          Stochastics.pdf},
}

@article{coverELEMENTSINFORMATIONTHEORY,
  title = {{{ELEMENTS OF INFORMATION THEORY}}},
  author = {Cover, Thomas M and Thomas, Joy A},
  langid = {english},
  file = {/home/kellen/Zotero/storage/C6AJGW5I/Cover and Thomas - ELEMENTS OF
          INFORMATION THEORY.pdf},
}

@misc{degrisStepsizeOptimizationContinual2024,
  title = {Step-Size {{Optimization}} for {{Continual Learning}}},
  author = {Degris, Thomas and Javed, Khurram and Sharifnassab, Arsalan and Liu,
            Yuxin and Sutton, Richard},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  urldate = {2024-09-06},
  abstract = {In continual learning, a learner has to keep learning from the
              data over its whole life time. A key issue is to decide what
              knowledge to keep and what knowledge to let go. In a neural network
              , this can be implemented by using a step-size vector to scale how
              much gradient samples change network weights. Common algorithms,
              like RMSProp and Adam, use heuristics, specifically normalization,
              to adapt this step-size vector. In this paper, we show that those
              heuristics ignore the effect of their adaptation on the overall
              objective function, for example by moving the step-size vector away
              from better step-size vectors. On the other hand, stochastic
              meta-gradient descent algorithms, like IDBD (Sutton, 1992),
              explicitly optimize the step-size vector with respect to the
              overall objective function. On simple problems, we show that IDBD
              is able to consistently improve step-size vectors, where RMSProp
              and Adam do not. We explain the differences between the two
              approaches and their respective limitations. We conclude by
              suggesting that combining both approaches could be a promising
              future direction to improve the performance of neural networks in
              continual learning.},
  howpublished = {https://arxiv.org/abs/2401.17401v1},
  langid = {english},
  file = {/home/kellen/Zotero/storage/YSGENB8H/Degris et al. - 2024 - Step-size
          Optimization for Continual Learning.pdf},
}

@book{diestelGraphTheory2017,
  title = {Graph {{Theory}}},
  author = {Diestel, Reinhard},
  year = {2017},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {173},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-53622-3},
  urldate = {2024-10-18},
  copyright = {
               https://www.springernature.com/gp/researchers/text-and-data-mining
               },
  isbn = {978-3-662-53621-6 978-3-662-53622-3},
  langid = {english},
  file = {/home/kellen/Zotero/storage/I3MHRRZZ/Diestel - 2017 - Graph Theory.pdf
          },
}

@book{durrettEssentialsStochasticProcesses2016,
  title = {Essentials of {{Stochastic Processes}}},
  author = {Durrett, Richard},
  year = {2016},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-45614-0},
  urldate = {2024-10-18},
  copyright = {
               https://www.springernature.com/gp/researchers/text-and-data-mining
               },
  isbn = {978-3-319-45613-3 978-3-319-45614-0},
  langid = {english},
  file = {/home/kellen/Zotero/storage/MVRAWH8Q/Durrett - 2016 - Essentials of
          Stochastic Processes.pdf},
}

@misc{eysenbachContrastiveLearningGoalConditioned2023,
  title = {Contrastive {{Learning}} as {{Goal-Conditioned Reinforcement Learning
           }}},
  author = {Eysenbach, Benjamin and Zhang, Tianjun and Salakhutdinov, Ruslan and
            Levine, Sergey},
  year = {2023},
  month = feb,
  number = {arXiv:2206.07568},
  eprint = {2206.07568},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.07568},
  urldate = {2024-09-06},
  abstract = {In reinforcement learning (RL), it is easier to solve a task if
              given a good representation. While deep RL should automatically
              acquire such good representations, prior work often finds that
              learning representations in an end-to-end fashion is unstable and
              instead equip RL algorithms with additional representation learning
              parts (e.g., auxiliary losses, data augmentation). How can we
              design RL algorithms that directly acquire good representations? In
              this paper, instead of adding representation learning parts to an
              existing RL algorithm, we show (contrastive) representation
              learning methods can be cast as RL algorithms in their own right.
              To do this, we build upon prior work and apply contrastive
              representation learning to action-labeled trajectories, in such a
              way that the (inner product of) learned representations exactly
              corresponds to a goal-conditioned value function. We use this idea
              to reinterpret a prior RL method as performing contrastive learning
              , and then use the idea to propose a much simpler method that
              achieves similar performance. Across a range of goal-conditioned RL
              tasks, we demonstrate that contrastive RL methods achieve higher
              success rates than prior non-contrastive methods, including in the
              offline RL setting. We also show that contrastive RL outperforms
              prior methods on image-based tasks, without using data augmentation
              or auxiliary objectives.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning},
  file = {/home/kellen/Zotero/storage/WVQKIMN4/Eysenbach et al. - 2023 -
          Contrastive Learning as Goal-Conditioned Reinforcement Learning.pdf},
}

@article{eysenbachProbabilisticReinforcementLearning,
  title = {Probabilistic {{Reinforcement Learning}}: {{Using Data}} to {{Define
           Desired Outcomes}} and {{Inferring How}} to {{Get There}}},
  author = {Eysenbach, Benjamin},
  langid = {english},
  file = {/home/kellen/Zotero/storage/PYCTKQX2/Eysenbach - Probabilistic
          Reinforcement Learning Using Data to Define Desired Outcomes and
          Inferring How to Get.pdf},
}

@misc{gomezProperLaplacianRepresentation2024,
  title = {Proper {{Laplacian Representation Learning}}},
  author = {Gomez, Diego and Bowling, Michael and Machado, Marlos C.},
  year = {2024},
  month = apr,
  number = {arXiv:2310.10833},
  eprint = {2310.10833},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-18},
  abstract = {The ability to learn good representations of states is essential
              for solving large reinforcement learning problems, where
              exploration, generalization, and transfer are particularly
              challenging. The Laplacian representation is a promising approach
              to address these problems by inducing informative state encoding
              and intrinsic rewards for temporally-extended action discovery and
              reward shaping. To obtain the Laplacian representation one needs to
              compute the eigensystem of the graph Laplacian, which is often
              approximated through optimization objectives compatible with deep
              learning approaches. These approximations, however, depend on
              hyperparameters that are impossible to tune efficiently, converge
              to arbitrary rotations of the desired eigenvectors, and are unable
              to accurately recover the corresponding eigenvalues. In this paper
              we introduce a theoretically sound objective and corresponding
              optimization algorithm for approximating the Laplacian
              representation. Our approach naturally recovers both the true
              eigenvectors and eigenvalues while eliminating the hyperparameter
              dependence of previous approximations. We provide theoretical
              guarantees for our method and we show that those results translate
              empirically into robust learning across multiple environments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning},
  file = {/home/kellen/Zotero/storage/AGETEYXM/Gomez et al. - 2024 - Proper
          Laplacian Representation Learning.pdf},
}

@article{guntherExaminingUseTemporalDifference2020,
  title = {Examining the {{Use}} of {{Temporal-Difference Incremental
           Delta-Bar-Delta}} for {{Real-World Predictive Knowledge Architectures}
           }},
  author = {G{\"u}nther, Johannes and Ady, Nadia M. and Kearney, Alex and Dawson
            , Michael R. and Pilarski, Patrick M.},
  year = {2020},
  month = mar,
  journal = {Frontiers in Robotics and AI},
  volume = {7},
  pages = {34},
  issn = {2296-9144},
  doi = {10.3389/frobt.2020.00034},
  urldate = {2024-10-29},
  langid = {english},
  file = {/home/kellen/Zotero/storage/3N55F4WQ/Günther et al. - 2020 - Examining
          the Use of Temporal-Difference Incremental Delta-Bar-Delta for
          Real-World Predictive Knowl.pdf},
}

@misc{guoDirectLanguageModel2024,
  title = {Direct {{Language Model Alignment}} from {{Online AI Feedback}}},
  author = {Guo, Shangmin and Zhang, Biao and Liu, Tianlin and Liu, Tianqi and
            Khalman, Misha and Llinares, Felipe and Rame, Alexandre and Mesnard,
            Thomas and Zhao, Yao and Piot, Bilal and Ferret, Johan and Blondel,
            Mathieu},
  year = {2024},
  month = feb,
  number = {arXiv:2402.04792},
  eprint = {2402.04792},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-11},
  abstract = {Direct alignment from preferences (DAP) methods, such as DPO, have
              recently emerged as efficient alternatives to reinforcement
              learning from human feedback (RLHF), that do not require a separate
              reward model. However, the preference datasets used in DAP methods
              are usually collected ahead of training and never updated, thus the
              feedback is purely offline. Moreover, responses in these datasets
              are often sampled from a language model distinct from the one being
              aligned, and since the model evolves over training, the alignment
              phase is inevitably off-policy. In this study, we posit that online
              feedback is key and improves DAP methods. Our method, online AI
              feedback (OAIF), uses an LLM as annotator: on each training
              iteration, we sample two responses from the current model and
              prompt the LLM annotator to choose which one is preferred, thus
              providing online feedback. Despite its simplicity, we demonstrate
              via human evaluation in several tasks that OAIF outperforms both
              offline DAP and RLHF methods. We further show that the feedback
              leveraged in OAIF is easily controllable, via instruction prompts
              to the LLM annotator.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science -
              Computation and Language,Computer Science - Human-Computer
              Interaction},
  file = {/home/kellen/Zotero/storage/GTI664DM/Guo et al. - 2024 - Direct
          Language Model Alignment from Online AI Feedback.pdf},
}

@misc{huangReinforcementLearningLowRank2023,
  title = {Reinforcement {{Learning}} in {{Low-Rank MDPs}} with {{Density
           Features}}},
  author = {Huang, Audrey and Chen, Jinglin and Jiang, Nan},
  year = {2023},
  month = feb,
  number = {arXiv:2302.02252},
  eprint = {2302.02252},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-09-16},
  abstract = {MDPs with low-rank transitions---that is, the transition matrix
              can be factored into the product of two matrices, left and
              right---is a highly representative structure that enables tractable
              learning. The left matrix enables expressive function approximation
              for value-based learning and has been studied extensively. In this
              work, we instead investigate sample-efficient learning with density
              features, i.e., the right matrix, which induce powerful models for
              state-occupancy distributions. This setting not only sheds light on
              leveraging unsupervised learning in RL, but also enables plug-in
              solutions for convex RL. In the offline setting, we propose an
              algorithm for off-policy estimation of occupancies that can handle
              non-exploratory data. Using this as a subroutine, we further devise
              an online algorithm that constructs exploratory data distributions
              in a level-by-level manner. As a central technical challenge, the
              additive error of occupancy estimation is incompatible with the
              multiplicative definition of data coverage. In the absence of
              strong assumptions like reachability, this incompatibility easily
              leads to exponential error blow-up, which we overcome via novel
              technical tools. Our results also readily extend to the
              representation learning setting, when the density features are
              unknown and must be learned from an exponentially large candidate
              set.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Statistics - Machine Learning},
  file = {/home/kellen/Zotero/storage/JV6TZ7S9/Huang et al. - 2023 -
          Reinforcement Learning in Low-Rank MDPs with Density Features.pdf},
}

@book{khanferFundamentalsFunctionalAnalysis2023,
  title = {Fundamentals of {{Functional Analysis}}},
  author = {Khanfer, Ammar},
  year = {2023},
  publisher = {Springer Nature Singapore},
  address = {Singapore},
  doi = {10.1007/978-981-99-3029-6},
  urldate = {2024-10-18},
  copyright = {
               https://www.springernature.com/gp/researchers/text-and-data-mining
               },
  isbn = {978-981-9930-28-9 978-981-9930-29-6},
  langid = {english},
  file = {/home/kellen/Zotero/storage/Z2NQSGWL/Khanfer - 2023 - Fundamentals of
          Functional Analysis.pdf},
}

@book{khanferMeasureTheoryIntegration2023,
  title = {Measure {{Theory}} and {{Integration}}},
  author = {Khanfer, Ammar},
  year = {2023},
  publisher = {Springer Nature Singapore},
  address = {Singapore},
  doi = {10.1007/978-981-99-2882-8},
  urldate = {2024-10-18},
  copyright = {
               https://www.springernature.com/gp/researchers/text-and-data-mining
               },
  isbn = {978-981-9928-81-1 978-981-9928-82-8},
  langid = {english},
  file = {/home/kellen/Zotero/storage/9CDPYJ3A/Khanfer - 2023 - Measure Theory
          and Integration.pdf},
}

@misc{kimUnsupervisedOnlineReinforcementLearning2024,
  title = {Unsupervised-to-{{Online Reinforcement Learning}}},
  author = {Kim, Junsu and Park, Seohong and Levine, Sergey},
  year = {2024},
  month = aug,
  journal = {arXiv.org},
  urldate = {2024-09-06},
  abstract = {Offline-to-online reinforcement learning (RL), a framework that
              trains a policy with offline RL and then further fine-tunes it with
              online RL, has been considered a promising recipe for data-driven
              decision-making. While sensible, this framework has drawbacks: it
              requires domain-specific offline RL pre-training for each task, and
              is often brittle in practice. In this work, we propose
              unsupervised-to-online RL (U2O RL), which replaces domain-specific
              supervised offline RL with unsupervised offline RL, as a better
              alternative to offline-to-online RL. U2O RL not only enables
              reusing a single pre-trained model for multiple downstream tasks,
              but also learns better representations, which often result in even
              better performance and stability than supervised offline-to-online
              RL. To instantiate U2O RL in practice, we propose a general recipe
              for U2O RL to bridge task-agnostic unsupervised offline skill-based
              policy pre-training and supervised online fine-tuning. Throughout
              our experiments in nine state-based and pixel-based environments,
              we empirically demonstrate that U2O RL achieves strong performance
              that matches or even outperforms previous offline-to-online RL
              approaches, while being able to reuse a single pre-trained model
              for a number of different downstream tasks.},
  howpublished = {https://arxiv.org/abs/2408.14785v1},
  langid = {english},
  file = {/home/kellen/Zotero/storage/FRV2CM6E/Kim et al. - 2024 -
          Unsupervised-to-Online Reinforcement Learning.pdf},
}

@misc{klissarovDeepLaplacianbasedOptions2023,
  title = {Deep {{Laplacian-based Options}} for {{Temporally-Extended
           Exploration}}},
  author = {Klissarov, Martin and Machado, Marlos C.},
  year = {2023},
  month = jun,
  number = {arXiv:2301.11181},
  eprint = {2301.11181},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-18},
  abstract = {Selecting exploratory actions that generate a rich stream of
              experience for better learning is a fundamental challenge in
              reinforcement learning (RL). An approach to tackle this problem
              consists in selecting actions according to specific policies for an
              extended period of time, also known as options. A recent line of
              work to derive such exploratory options builds upon the
              eigenfunctions of the graph Laplacian. Importantly, until now these
              methods have been mostly limited to tabular domains where (1) the
              graph Laplacian matrix was either given or could be fully estimated
              , (2) performing eigendecomposition on this matrix was
              computationally tractable, and (3) value functions could be learned
              exactly. Additionally, these methods required a separate option
              discovery phase. These assumptions are fundamentally not scalable.
              In this paper we address these limitations and show how recent
              results for directly approximating the eigenfunctions of the
              Laplacian can be leveraged to truly scale up options-based
              exploration. To do so, we introduce a fully online deep RL
              algorithm for discovering Laplacianbased options and evaluate our
              approach on a variety of pixel-based tasks. We compare to several
              state-of-the-art exploration methods and show that our approach is
              effective, general, and especially promising in non-stationary
              settings.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning},
  file = {/home/kellen/Zotero/storage/RJ3AYCQA/Klissarov and Machado - 2023 -
          Deep Laplacian-based Options for Temporally-Extended Exploration.pdf},
}

@misc{kumarDR3ValueBasedDeep2021,
  title = {{{DR3}}: {{Value-Based Deep Reinforcement Learning Requires Explicit
           Regularization}}},
  shorttitle = {{{DR3}}},
  author = {Kumar, Aviral and Agarwal, Rishabh and Ma, Tengyu and Courville,
            Aaron and Tucker, George and Levine, Sergey},
  year = {2021},
  month = dec,
  number = {arXiv:2112.04716},
  eprint = {2112.04716},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-11},
  abstract = {Despite overparameterization, deep networks trained via supervised
              learning are easy to optimize and exhibit excellent generalization.
              One hypothesis to explain this is that overparameterized deep
              networks enjoy the benefits of implicit regularization induced by
              stochastic gradient descent, which favors parsimonious solutions
              that generalize well on test inputs. It is reasonable to surmise
              that deep reinforcement learning (RL) methods could also benefit
              from this effect. In this paper, we discuss how the implicit
              regularization effect of SGD seen in supervised learning could in
              fact be harmful in the offline deep RL setting, leading to poor
              generalization and degenerate feature representations. Our
              theoretical analysis shows that when existing models of implicit
              regularization are applied to temporal difference learning, the
              resulting derived regularizer favors degenerate solutions with
              excessive ``aliasing'', in stark contrast to the supervised
              learning case. We back up these findings empirically, showing that
              feature representations learned by a deep network value function
              trained via bootstrapping can indeed become degenerate, aliasing
              the representations for state-action pairs that appear on either
              side of the Bellman backup. To address this issue, we derive the
              form of this implicit regularizer and, inspired by this derivation,
              propose a simple and effective explicit regularizer, called DR3,
              that counteracts the undesirable effects of this implicit
              regularizer. When combined with existing offline RL methods, DR3
              substantially improves performance and stability, alleviating
              unlearning in Atari 2600 games, D4RL domains and robotic
              manipulation from images.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/kellen/Zotero/storage/S9MJBMRM/Kumar et al. - 2021 - DR3
          Value-Based Deep Reinforcement Learning Requires Explicit
          Regularization.pdf},
}

@misc{langTheoreticalAnalysisWeakStrong2024,
  title = {Theoretical {{Analysis}} of {{Weak-to-Strong Generalization}}},
  author = {Lang, Hunter and Sontag, David and Vijayaraghavan, Aravindan},
  year = {2024},
  month = may,
  number = {arXiv:2405.16043},
  eprint = {2405.16043},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-09-18},
  abstract = {Strong student models can learn from weaker teachers: when trained
              on the predictions of a weaker model, a strong pretrained student
              can learn to correct the weak model's errors and generalize to
              examples where the teacher is not confident, even when these
              examples are excluded from training. This enables learning from
              cheap, incomplete, and possibly incorrect label information, such
              as coarse logical rules or the generations of a language model. We
              show that existing weak supervision theory fails to account for
              both of these effects, which we call pseudolabel correction and
              coverage expansion, respectively. We give a new bound based on
              expansion properties of the data distribution and student
              hypothesis class that directly accounts for pseudolabel correction
              and coverage expansion. Our bounds capture the intuition that
              weak-to-strong generalization occurs when the strong model is
              unable to fit the mistakes of the weak teacher without incurring
              additional error. We show that these expansion properties can be
              checked from finite data and give empirical evidence that they hold
              in practice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science -
              Machine Learning,Statistics - Machine Learning},
  file = {/home/kellen/Zotero/storage/LJQ96YTP/Lang et al. - 2024 - Theoretical
          Analysis of Weak-to-Strong Generalization.pdf},
}

@book{lattimoreBanditAlgorithms2020,
  title = {Bandit {{Algorithms}}},
  author = {Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year = {2020},
  month = jul,
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/9781108571401},
  urldate = {2024-10-24},
  copyright = {https://www.cambridge.org/core/terms},
  isbn = {978-1-108-57140-1 978-1-108-48682-8},
  langid = {english},
  file = {/home/kellen/Zotero/storage/U3N2WASX/Lattimore and Szepesvári - 2020 -
          Bandit Algorithms.pdf},
}

@book{leeIntroductionSmoothManifolds2012,
  title = {Introduction to {{Smooth Manifolds}}},
  author = {Lee, John M.},
  year = {2012},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {218},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4419-9982-5},
  urldate = {2024-10-18},
  copyright = {
               https://www.springernature.com/gp/researchers/text-and-data-mining
               },
  isbn = {978-1-4419-9981-8 978-1-4419-9982-5},
  langid = {english},
  file = {/home/kellen/Zotero/storage/GFNZMLM6/Lee - 2012 - Introduction to
          Smooth Manifolds.pdf},
}

@misc{leeRLAIFVsRLHF2024,
  title = {{{RLAIF}} vs. {{RLHF}}: {{Scaling Reinforcement Learning}} from {{
           Human Feedback}} with {{AI Feedback}}},
  shorttitle = {{{RLAIF}} vs. {{RLHF}}},
  author = {Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard,
            Thomas and Ferret, Johan and Lu, Kellie and Bishop, Colton and Hall,
            Ethan and Carbune, Victor and Rastogi, Abhinav and Prakash, Sushant},
  year = {2024},
  month = sep,
  number = {arXiv:2309.00267},
  eprint = {2309.00267},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-11},
  abstract = {Reinforcement learning from human feedback (RLHF) has proven
              effective in aligning large language models (LLMs) with human
              preferences, but gathering high-quality preference labels is
              expensive. RL from AI Feedback (RLAIF), introduced in Bai et al.
              (2022b), offers a promising alternative that trains the reward
              model (RM) on preferences generated by an off-the-shelf LLM. Across
              the tasks of summarization, helpful dialogue generation, and
              harmless dialogue generation, we show that RLAIF achieves
              comparable performance to RLHF. Furthermore, we take a step towards
              ``self-improvement'' by demonstrating that RLAIF can outperform a
              supervised finetuned baseline even when the AI labeler is the same
              size as the policy, or even the exact same checkpoint as the
              initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a
              technique that circumvents RM training by obtaining rewards
              directly from an off-the-shelf LLM during RL, which achieves
              superior performance to canonical RLAIF. Our results suggest that
              RLAIF can achieve performance on-par with using human feedback,
              offering a potential solution to the scalability limitations of
              RLHF.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science -
              Computation and Language,Computer Science - Machine Learning},
  file = {/home/kellen/Zotero/storage/IWPVFZCM/Lee et al. - 2024 - RLAIF vs.
          RLHF Scaling Reinforcement Learning from Human Feedback with AI
          Feedback.pdf},
}

@misc{lewandowskiLearningContinuallySpectral2024,
  title = {Learning {{Continually}} by {{Spectral Regularization}}},
  author = {Lewandowski, Alex and Kumar, Saurabh and Schuurmans, Dale and Gy{\"o
            }rgy, Andr{\'a}s and Machado, Marlos C.},
  year = {2024},
  month = jun,
  number = {arXiv:2406.06811},
  eprint = {2406.06811},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-18},
  abstract = {Loss of plasticity is a phenomenon where neural networks become
              more difficult to train during the course of learning. Continual
              learning algorithms seek to mitigate this effect by sustaining good
              predictive performance while maintaining network trainability. We
              develop new techniques for improving continual learning by first
              reconsidering how initialization can ensure trainability during
              early phases of learning. From this perspective, we derive new
              regularization strategies for continual learning that ensure
              beneficial initialization properties are better maintained
              throughout training. In particular, we investigate two new
              regularization techniques for continual learning: (i) Wasserstein
              regularization toward the initial weight distribution, which is
              less restrictive than regularizing toward initial weights; and (ii)
              regularizing weight matrix singular values, which directly ensures
              gradient diversity is maintained throughout training. We present an
              experimental analysis that shows these alternative regularizers can
              improve continual learning performance across a range of supervised
              learning tasks and model architectures. The alternative
              regularizers prove to be less sensitive to hyperparameters while
              demonstrating better training in individual tasks, sustaining
              trainability as new tasks arrive, and achieving better
              generalization performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/kellen/Zotero/storage/6HX2DA95/Lewandowski et al. - 2024 -
          Learning Continually by Spectral Regularization.pdf},
}

@article{liDeepLearningCompiler2021,
  title = {The {{Deep Learning Compiler}}: {{A Comprehensive Survey}}},
  shorttitle = {The {{Deep Learning Compiler}}},
  author = {Li, Mingzhen and Liu, Yi and Liu, Xiaoyan and Sun, Qingxiao and You,
            Xin and Yang, Hailong and Luan, Zhongzhi and Gan, Lin and Yang,
            Guangwen and Qian, Depei},
  year = {2021},
  month = mar,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {32},
  number = {3},
  eprint = {2002.03794},
  primaryclass = {cs},
  pages = {708--727},
  issn = {1045-9219, 1558-2183, 2161-9883},
  doi = {10.1109/TPDS.2020.3030548},
  urldate = {2024-10-20},
  abstract = {The difficulty of deploying various deep learning (DL) models on
              diverse DL hardware has boosted the research and development of DL
              compilers in the community. Several DL compilers have been proposed
              from both industry and academia such as Tensorflow XLA and TVM.
              Similarly, the DL compilers take the DL models described in
              different DL frameworks as input, and then generate optimized codes
              for diverse DL hardware as output. However, none of the existing
              survey has analyzed the unique design architecture of the DL
              compilers comprehensively. In this paper, we perform a
              comprehensive survey of existing DL compilers by dissecting the
              commonly adopted design in details, with emphasis on the DL
              oriented multi-level IRs, and frontend/backend optimizations. We
              present detailed analysis on the design of multi-level IRs and
              illustrate the commonly adopted optimization techniques. Finally,
              several insights are highlighted as the potential research
              directions of DL compiler. This is the first survey paper focusing
              on the design architecture of DL compilers, which we hope can pave
              the road for future research towards DL compiler.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,
              Computer Science - Machine Learning,Computer Science - Performance},
  file = {/home/kellen/Zotero/storage/TLNQRMTF/Li et al. - 2021 - The Deep
          Learning Compiler A Comprehensive Survey.pdf},
}

@misc{liuSingleGoalAll2024,
  title = {A {{Single Goal}} Is {{All You Need}}: {{Skills}} and {{Exploration
           Emerge}} from {{Contrastive RL}} without {{Rewards}}, {{Demonstrations
           }}, or {{Subgoals}}},
  shorttitle = {A {{Single Goal}} Is {{All You Need}}},
  author = {Liu, Grace and Tang, Michael and Eysenbach, Benjamin},
  year = {2024},
  month = aug,
  number = {arXiv:2408.05804},
  eprint = {2408.05804},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.05804},
  urldate = {2024-09-06},
  abstract = {In this paper, we present empirical evidence of skills and
              directed exploration emerging from a simple RL algorithm long
              before any successful trials are observed. For example, in a
              manipulation task, the agent is given a single observation of the
              goal state and learns skills, first for moving its end-effector,
              then for pushing the block, and finally for picking up and placing
              the block. These skills emerge before the agent has ever
              successfully placed the block at the goal location and without the
              aid of any reward functions, demonstrations, or manually-specified
              distance metrics. Once the agent has learned to reach the goal
              state reliably, exploration is reduced. Implementing our method
              involves a simple modification of prior work and does not require
              density estimates, ensembles, or any additional hyperparameters.
              Intuitively, the proposed method seems like it should be terrible
              at exploration, and we lack a clear theoretical understanding of
              why it works so effectively, though our experiments provide some
              hints.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning},
  file = {/home/kellen/Zotero/storage/U8KGMTC6/Liu et al. - 2024 - A Single Goal
          is All You Need Skills and Exploration Emerge from Contrastive RL
          without Rewards, De.pdf},
}

@misc{machadoTemporalAbstractionReinforcement2023,
  title = {Temporal {{Abstraction}} in {{Reinforcement Learning}} with the {{
           Successor Representation}}},
  author = {Machado, Marlos C. and Barreto, Andre and Precup, Doina and Bowling,
            Michael},
  year = {2023},
  month = apr,
  number = {arXiv:2110.05740},
  eprint = {2110.05740},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-18},
  abstract = {Reasoning at multiple levels of temporal abstraction is one of the
              key attributes of intelligence. In reinforcement learning, this is
              often modeled through temporally extended courses of actions called
              options. Options allow agents to make predictions and to operate at
              different levels of abstraction within an environment. Nevertheless
              , approaches based on the options framework often start with the
              assumption that a reasonable set of options is known beforehand.
              When this is not the case, there are no definitive answers for
              which options one should consider. In this paper, we argue that the
              successor representation, which encodes states based on the pattern
              of state visitation that follows them, can be seen as a natural
              substrate for the discovery and use of temporal abstractions. To
              support our claim, we take a big picture view of recent results,
              showing how the successor representation can be used to discover
              options that facilitate either temporally-extended exploration or
              planning. We cast these results as instantiations of a general
              framework for option discovery in which the agent's representation
              is used to identify useful options, which are then used to further
              improve its representation. This results in a virtuous,
              never-ending, cycle in which both the representation and the
              options are constantly refined based on each other. Beyond option
              discovery itself, we also discuss how the successor representation
              allows us to augment a set of options into a combinatorially large
              counterpart without additional learning. This is achieved through
              the combination of previously learned options. Our empirical
              evaluation focuses on options discovered for temporally-extended
              exploration and on the use of the successor representation to
              combine them. Our results shed light on important design decisions
              involved in the definition of options and demonstrate the synergy
              of different methods based on the successor representation, such as
              eigenoptions and the option keyboard.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning},
  file = {/home/kellen/Zotero/storage/PEJ6S5ZX/Machado et al. - 2023 - Temporal
          Abstraction in Reinforcement Learning with the Successor
          Representation.pdf},
}

@misc{malladiFineTuningLanguageModels2024,
  title = {Fine-{{Tuning Language Models}} with {{Just Forward Passes}}},
  author = {Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian,
            Alex and Lee, Jason D. and Chen, Danqi and Arora, Sanjeev},
  year = {2024},
  month = jan,
  number = {arXiv:2305.17333},
  eprint = {2305.17333},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.17333},
  urldate = {2024-09-08},
  abstract = {Fine-tuning language models (LMs) has yielded success on diverse
              downstream tasks, but as LMs grow in size, backpropagation requires
              a prohibitively large amount of memory. Zeroth-order (ZO) methods
              can in principle estimate gradients using only two forward passes
              but are theorized to be catastrophically slow for optimizing large
              models. In this work, we propose a memory-efficient zerothorder
              optimizer (MeZO), adapting the classical ZO-SGD method to operate
              in-place, thereby fine-tuning LMs with the same memory footprint as
              inference. For example, with a single A100 80GB GPU, MeZO can train
              a 30-billion parameter model, whereas fine-tuning with
              backpropagation can train only a 2.7B LM with the same budget. We
              conduct comprehensive experiments across model types (masked and
              autoregressive LMs), model scales (up to 66B), and downstream tasks
              (classification, multiple-choice, and generation). Our results
              demonstrate that (1) MeZO significantly outperforms in-context
              learning and linear probing; (2) MeZO achieves comparable
              performance to fine-tuning with backpropagation across multiple
              tasks, with up to 12x memory reduction and up to 2x GPU-hour
              reduction in our implementation; (3) MeZO is compatible with both
              full-parameter and parameter-efficient tuning techniques such as
              LoRA and prefix tuning; (4) MeZO can effectively optimize
              non-differentiable objectives (e.g., maximizing accuracy or F1). We
              support our empirical findings with theoretical insights,
              highlighting how adequate pre-training and task prompts enable MeZO
              to fine-tune huge models, despite classical ZO analyses suggesting
              otherwise.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science -
              Machine Learning},
  file = {/home/kellen/Zotero/storage/2J3FEQX7/Malladi et al. - 2024 -
          Fine-Tuning Language Models with Just Forward Passes.pdf},
}

@misc{mcleodContinualAuxiliaryTask2022,
  title = {Continual {{Auxiliary Task Learning}}},
  author = {McLeod, Matthew and Lo, Chunlok and Schlegel, Matthew and Jacobsen,
            Andrew and Kumaraswamy, Raksha and White, Martha and White, Adam},
  year = {2022},
  month = feb,
  number = {arXiv:2202.11133},
  eprint = {2202.11133},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-16},
  abstract = {Learning auxiliary tasks, such as multiple predictions about the
              world, can provide many benefits to reinforcement learning systems.
              A variety of off-policy learning algorithms have been developed to
              learn such predictions, but as yet there is little work on how to
              adapt the behavior to gather useful data for those off-policy
              predictions. In this work, we investigate a reinforcement learning
              system designed to learn a collection of auxiliary tasks, with a
              behavior policy learning to take actions to improve those auxiliary
              predictions. We highlight the inherent non-stationarity in this
              continual auxiliary task learning problem, for both prediction
              learners and the behavior learner. We develop an algorithm based on
              successor features that facilitates tracking under non-stationary
              rewards, and prove the separation into learning successor features
              and rewards provides convergence rate improvements. We conduct an
              in-depth study into the resulting multi-prediction learning system.
              },
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/kellen/Zotero/storage/GUALHQEZ/McLeod et al. - 2022 - Continual
          Auxiliary Task Learning.pdf},
}

@misc{mrouehInformationTheoreticGuarantees2024,
  title = {Information {{Theoretic Guarantees For Policy Alignment In Large
           Language Models}}},
  author = {Mroueh, Youssef},
  year = {2024},
  month = jun,
  number = {arXiv:2406.05883},
  eprint = {2406.05883},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-09-18},
  abstract = {Policy alignment of large language models refers to constrained
              policy optimization, where the policy is optimized to maximize a
              reward while staying close to a reference policy with respect to an
              \$f\$-divergence such as the \${\textbackslash}mathsf\{KL\}\$
              divergence. The best of \$n\$ alignment policy selects a sample
              from the reference policy that has the maximum reward among \$n\$
              independent samples. For both cases (policy alignment and best of
              \$n\$), recent works showed empirically that the reward improvement
              of the aligned policy on the reference one scales like \${
              \textbackslash}sqrt\{{\textbackslash}mathsf\{KL\}\}\$, with an
              explicit bound in \$n\$ on the \${\textbackslash}mathsf\{KL\}\$ for
              the best of \$n\$ policy. We show in this paper that the \${
              \textbackslash}sqrt\{{\textbackslash}mathsf\{KL\}\}\$ information
              theoretic upper bound holds if the reward under the reference
              policy has sub-gaussian tails. Moreover, we prove for the best of
              \$n\$ policy, that the \${\textbackslash}mathsf\{KL\}\$ upper bound
              can be obtained for any \$f\$-divergence via a reduction to
              exponential order statistics owing to the R{\textbackslash}'enyi
              representation of order statistics, and a data processing
              inequality. If additional information is known on the tails of the
              aligned policy we show that tighter control on the reward
              improvement can be obtained via the R{\textbackslash}'enyi
              divergence. Finally we demonstrate how these upper bounds transfer
              from proxy rewards to golden rewards which results in a decrease in
              the golden reward improvement due to overestimation and
              approximation errors of the proxy reward.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Computer Science - Machine
              Learning,Statistics - Machine Learning},
  file = {/home/kellen/Zotero/storage/5NKQB3ZU/Mroueh - 2024 - Information
          Theoretic Guarantees For Policy Alignment In Large Language Models.pdf},
}

@misc{myersLearningTemporalDistances2024,
  title = {Learning {{Temporal Distances}}: {{Contrastive Successor Features Can
           Provide}} a {{Metric Structure}} for {{Decision-Making}}},
  shorttitle = {Learning {{Temporal Distances}}},
  author = {Myers, Vivek and Zheng, Chongyi and Dragan, Anca and Levine, Sergey
            and Eysenbach, Benjamin},
  year = {2024},
  month = jun,
  number = {arXiv:2406.17098},
  eprint = {2406.17098},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.17098},
  urldate = {2024-09-06},
  abstract = {Temporal distances lie at the heart of many algorithms for
              planning, control, and reinforcement learning that involve reaching
              goals, allowing one to estimate the transit time between two
              states. However, prior attempts to define such temporal distances
              in stochastic settings have been stymied by an important
              limitation: these prior approaches do not satisfy the triangle
              inequality. This is not merely a definitional concern, but
              translates to an inability to generalize and find shortest paths.
              In this paper, we build on prior work in contrastive learning and
              quasimetrics to show how successor features learned by contrastive
              learning (after a change of variables) form a temporal distance
              that does satisfy the triangle inequality, even in stochastic
              settings. Importantly, this temporal distance is computationally
              efficient to estimate, even in high-dimensional and stochastic
              settings. Experiments in controlled settings and benchmark suites
              demonstrate that an RL algorithm based on these new temporal
              distances exhibits combinatorial generalization (i.e., "stitching")
              and can sometimes learn more quickly than prior methods, including
              those based on quasimetrics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning},
  file = {/home/kellen/Zotero/storage/X3LR92NR/Myers et al. - 2024 - Learning
          Temporal Distances Contrastive Successor Features Can Provide a Metric
          Structure for Decis.pdf},
}

@misc{nachumDataEfficientHierarchicalReinforcement2018,
  title = {Data-{{Efficient Hierarchical Reinforcement Learning}}},
  author = {Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  year = {2018},
  month = oct,
  number = {arXiv:1805.08296},
  eprint = {1805.08296},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-24},
  abstract = {Hierarchical reinforcement learning (HRL) is a promising approach
              to extend traditional reinforcement learning (RL) methods to solve
              more complex tasks. Yet, the majority of current HRL methods
              require careful task-specific design and on-policy training, making
              them difficult to apply in real-world scenarios. In this paper, we
              study how we can develop HRL algorithms that are general, in that
              they do not make onerous additional assumptions beyond standard RL
              algorithms, and efficient, in the sense that they can be used with
              modest numbers of interaction samples, making them suitable for
              real-world problems such as robotic control. For generality, we
              develop a scheme where lower-level controllers are supervised with
              goals that are learned and proposed automatically by the
              higher-level controllers. To address efficiency, we propose to use
              off-policy experience for both higher and lower-level training.
              This poses a considerable challenge, since changes to the
              lower-level behaviors change the action space for the higher-level
              policy, and we introduce an off-policy correction to remedy this
              challenge. This allows us to take advantage of recent advances in
              off-policy model-free RL to learn both higher- and lower-level
              policies using substantially fewer environment interactions than
              on-policy algorithms. We term the resulting HRL agent HIRO and find
              that it is generally applicable and highly sample-efficient. Our
              experiments show that HIRO can be used to learn highly complex
              behaviors for simulated robots, such as pushing objects and
              utilizing them to reach target locations, learning from only a few
              million samples, equivalent to a few days of real-time interaction.
              In comparisons with a number of prior HRL methods, we find that our
              approach substantially outperforms previous state-of-the-art
              techniques.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Statistics - Machine Learning},
  file = {/home/kellen/Zotero/storage/E5S647TN/Nachum et al. - 2018 -
          Data-Efficient Hierarchical Reinforcement Learning.pdf},
}

@inproceedings{NEURIPS2020_1457c0d6,
  title = {Language Models Are Few-Shot Learners},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie
            and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind
            and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal,
            Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan
            , Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu,
            Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and
            Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin
            and Clark, Jack and Berner, Christopher and McCandlish, Sam and
            Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and
            Lin, H.},
  year = {2020},
  volume = {33},
  pages = {1877--1901},
  publisher = {Curran Associates, Inc.},
}

@book{nocedalNumericalOptimization2006,
  title = {Numerical Optimization},
  author = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer Series in Operations Research},
  edition = {2nd ed},
  publisher = {Springer},
  address = {New York},
  isbn = {978-0-387-30303-1},
  langid = {english},
  lccn = {QA402.5 .N62 2006},
  keywords = {Mathematical optimization},
  annotation = {OCLC: ocm68629100},
  file = {/home/kellen/Zotero/storage/ZNRAHE63/Nocedal and Wright - 2006 -
          Numerical optimization.pdf},
}

@misc{parkFoundationPoliciesHilbert2024,
  title = {Foundation {{Policies}} with {{Hilbert Representations}}},
  author = {Park, Seohong and Kreiman, Tobias and Levine, Sergey},
  year = {2024},
  month = may,
  number = {arXiv:2402.15567},
  eprint = {2402.15567},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.15567},
  urldate = {2024-09-18},
  abstract = {Unsupervised and self-supervised objectives, such as next token
              prediction, have enabled pre-training generalist models from large
              amounts of unlabeled data. In reinforcement learning (RL), however,
              finding a truly general and scalable unsupervised pre-training
              objective for generalist policies from offline data remains a major
              open question. While a number of methods have been proposed to
              enable generic self-supervised RL, based on principles such as
              goal-conditioned RL, behavioral cloning, and unsupervised skill
              learning, such methods remain limited in terms of either the
              diversity of the discovered behaviors, the need for high-quality
              demonstration data, or the lack of a clear adaptation mechanism for
              downstream tasks. In this work, we propose a novel unsupervised
              framework to pre-train generalist policies that capture diverse,
              optimal, long-horizon behaviors from unlabeled offline data such
              that they can be quickly adapted to any arbitrary new tasks in a
              zero-shot manner. Our key insight is to learn a structured
              representation that preserves the temporal structure of the
              underlying environment, and then to span this learned latent space
              with directional movements, which enables various zero-shot policy
              "prompting" schemes for downstream tasks. Through our experiments
              on simulated robotic locomotion and manipulation benchmarks, we
              show that our unsupervised policies can solve goal-conditioned and
              general RL tasks in a zero-shot fashion, even often outperforming
              prior methods designed specifically for each setting. Our code and
              videos are available at https://seohong.me/projects/hilp/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Computer Science - Robotics},
  file = {/home/kellen/Zotero/storage/YNKYPBF3/Park et al. - 2024 - Foundation
          Policies with Hilbert
          Representations.pdf;/home/kellen/Zotero/storage/5H2B6U6Y/2402.html},
}

@misc{parkFoundationPoliciesHilbert2024a,
  title = {Foundation {{Policies}} with {{Hilbert Representations}}},
  author = {Park, Seohong and Kreiman, Tobias and Levine, Sergey},
  year = {2024},
  month = may,
  number = {arXiv:2402.15567},
  eprint = {2402.15567},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-16},
  abstract = {Unsupervised and self-supervised objectives, such as next token
              prediction, have enabled pretraining generalist models from large
              amounts of unlabeled data. In reinforcement learning (RL), however,
              finding a truly general and scalable unsupervised pre-training
              objective for generalist policies from offline data remains a major
              open question. While a number of methods have been proposed to
              enable generic self-supervised RL, based on principles such as
              goal-conditioned RL, behavioral cloning, and unsupervised skill
              learning, such methods remain limited in terms of either the
              diversity of the discovered behaviors, the need for high-quality
              demonstration data, or the lack of a clear adaptation mechanism for
              downstream tasks. In this work, we propose a novel unsupervised
              framework to pre-train generalist policies that capture diverse,
              optimal, long-horizon behaviors from unlabeled offline data such
              that they can be quickly adapted to any arbitrary new tasks in a
              zero-shot manner. Our key insight is to learn a structured
              representation that preserves the temporal structure of the
              underlying environment, and then to span this learned latent space
              with directional movements, which enables various zero-shot policy
              ``prompting'' schemes for downstream tasks. Through our experiments
              on simulated robotic locomotion and manipulation benchmarks, we
              show that our unsupervised policies can solve goal-conditioned and
              general RL tasks in a zero-shot fashion, even often outperforming
              prior methods designed specifically for each setting. Our code and
              videos are available at https: //seohong.me/projects/hilp/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Computer Science - Robotics},
  file = {/home/kellen/Zotero/storage/WEHZ63LJ/Park et al. - 2024 - Foundation
          Policies with Hilbert Representations.pdf},
}

@misc{qiOnlineDPOOnline2024,
  title = {Online {{DPO}}: {{Online Direct Preference Optimization}} with {{
           Fast-Slow Chasing}}},
  shorttitle = {Online {{DPO}}},
  author = {Qi, Biqing and Li, Pengfei and Li, Fangyuan and Gao, Junqi and Zhang
            , Kaiyan and Zhou, Bowen},
  year = {2024},
  month = jun,
  number = {arXiv:2406.05534},
  eprint = {2406.05534},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.05534},
  urldate = {2024-09-08},
  abstract = {Direct Preference Optimization (DPO) improves the alignment of
              large language models (LLMs) with human values by training directly
              on human preference datasets, eliminating the need for reward
              models. However, due to the presence of cross-domain human
              preferences, direct continual training can lead to catastrophic
              forgetting, limiting DPO's performance and efficiency. Inspired by
              intraspecific competition driving species evolution, we propose a
              Online Fast-Slow chasing DPO (OFS-DPO) for preference alignment,
              simulating competition through fast and slow chasing among models
              to facilitate rapid adaptation. Specifically, we first derive the
              regret upper bound for online learning, validating our motivation
              with a min-max optimization pattern. Based on this, we introduce
              two identical modules using Low-rank Adaptive (LoRA) with different
              optimization speeds to simulate intraspecific competition, and
              propose a new regularization term to guide their learning. To
              further mitigate catastrophic forgetting in cross-domain scenarios,
              we extend the OFS-DPO with LoRA modules combination strategy,
              resulting in the Cross domain Online Fast-Slow chasing DPO
              (COFS-DPO). This method leverages linear combinations of fast
              modules parameters from different task domains, fully utilizing
              historical information to achive continual value alignment.
              Experimental results show that OFS-DPO outperforms DPO in in-domain
              alignment, while COFS-DPO excels in cross-domain continual learning
              scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science -
              Computation and Language,Computer Science - Machine Learning},
  file = {/home/kellen/Zotero/storage/74CS8AIC/Qi et al. - 2024 - Online DPO
          Online Direct Preference Optimization with Fast-Slow Chasing.pdf},
}

@misc{rajbhandariZeROMemoryOptimizations2020,
  title = {{{ZeRO}}: {{Memory Optimizations Toward Training Trillion Parameter
           Models}}},
  shorttitle = {{{ZeRO}}},
  author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He,
            Yuxiong},
  year = {2020},
  month = may,
  number = {arXiv:1910.02054},
  eprint = {1910.02054},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-09-18},
  abstract = {Large deep learning models offer significant accuracy gains, but
              training billions to trillions of parameters is challenging.
              Existing solutions such as data and model parallelisms exhibit
              fundamental limitations to fit these models into limited device
              memory, while obtaining computation, communication and development
              efficiency. We develop a novel solution, Zero Redundancy Optimizer
              (ZeRO), to optimize memory, vastly improving training speed while
              increasing the model size that can be efficiently trained. ZeRO
              eliminates memory redundancies in data- and model-parallel training
              while retaining low communication volume and high computational
              granularity, allowing us to scale the model size proportional to
              the number of devices with sustained high efficiency. Our analysis
              on memory requirements and communication volume demonstrates: ZeRO
              has the potential to scale beyond 1 Trillion parameters using
              today's hardware.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,
              Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/kellen/Zotero/storage/FDXIZE3Y/Rajbhandari et al. - 2020 - ZeRO
          Memory Optimizations Toward Training Trillion Parameter Models.pdf},
}

@misc{sanz-alonsoFirstCourseMonte2024,
  title = {A {{First Course}} in {{Monte Carlo Methods}}},
  author = {{Sanz-Alonso}, Daniel and {Al-Ghattas}, Omar},
  year = {2024},
  month = may,
  number = {arXiv:2405.16359},
  eprint = {2405.16359},
  primaryclass = {stat},
  publisher = {arXiv},
  urldate = {2024-10-28},
  abstract = {This is a concise mathematical introduction to Monte Carlo methods
              , a rich family of algorithms with far-reaching applications in
              science and engineering. Monte Carlo methods are an exciting
              subject for mathematical statisticians and computational and
              applied mathematicians: the design and analysis of modern
              algorithms are rooted in a broad mathematical toolbox that includes
              ergodic theory of Markov chains, Hamiltonian dynamical systems,
              transport maps, stochastic differential equations, information
              theory, optimization, Riemannian geometry, and gradient flows,
              among many others. These lecture notes celebrate the breadth of
              mathematical ideas that have led to tangible advancements in Monte
              Carlo methods and their applications. To accommodate a diverse
              audience, the level of mathematical rigor varies from chapter to
              chapter, giving only an intuitive treatment to the most technically
              demanding subjects. The aim is not to be comprehensive or
              encyclopedic, but rather to illustrate some key principles in the
              design and analysis of Monte Carlo methods through a
              carefully-crafted choice of topics that emphasizes timeless over
              timely ideas. Algorithms are presented in a way that is conducive
              to conceptual understanding and mathematical analysis -- clarity
              and intuition are favored over state-of-the-art implementations
              that are harder to comprehend or rely on ad-hoc heuristics. To help
              readers navigate the expansive landscape of Monte Carlo methods,
              each algorithm is accompanied by a summary of its pros and cons,
              and by a discussion of the type of problems for which they are most
              useful. The presentation is self-contained, and therefore adequate
              for self-guided learning or as a teaching resource. Each chapter
              contains a section with bibliographic remarks that will be useful
              for those interested in conducting research on Monte Carlo methods
              and their applications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Numerical Analysis,Mathematics - History and
              Overview,Mathematics - Numerical Analysis,Statistics - Computation},
  file = {/home/kellen/Zotero/storage/YQNKW8C5/Sanz-Alonso and Al-Ghattas - 2024
          - A First Course in Monte Carlo Methods.pdf},
}

@misc{shoeybiMegatronLMTrainingMultiBillion2020,
  title = {Megatron-{{LM}}: {{Training Multi-Billion Parameter Language Models
           Using Model Parallelism}}},
  shorttitle = {Megatron-{{LM}}},
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley,
            Patrick and Casper, Jared and Catanzaro, Bryan},
  year = {2020},
  month = mar,
  number = {arXiv:1909.08053},
  eprint = {1909.08053},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-18},
  abstract = {Recent work in language modeling demonstrates that training large
              transformer models advances the state of the art in Natural
              Language Processing applications. However, very large models can be
              quite difficult to train due to memory constraints. In this work,
              we present our techniques for training very large transformer
              models and implement a simple, efficient intra-layer model parallel
              approach that enables training transformer models with billions of
              parameters. Our approach does not require a new compiler or library
              changes, is orthogonal and complimentary to pipeline model
              parallelism, and can be fully implemented with the insertion of a
              few communication operations in native PyTorch. We illustrate this
              approach by converging transformer based models up to 8.3 billion
              parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the
              entire application with 76\% scaling efficiency when compared to a
              strong single GPU baseline that sustains 39 TeraFLOPs, which is 30
              \% of peak FLOPs. To demonstrate that large language models can
              further advance the state of the art (SOTA), we train an 8.3
              billion parameter transformer language model similar to GPT-2 and a
              3.9 billion parameter model similar to BERT. We show that careful
              attention to the placement of layer normalization in BERT-like
              models is critical to achieving increased performance as the model
              size grows. Using the GPT-2 model we achieve SOTA results on the
              WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA
              (66.5\% compared to SOTA accuracy of 63.2\%) datasets. Our BERT
              model achieves SOTA results on the RACE dataset (90.9\% compared to
              SOTA accuracy of 89.4\%).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/kellen/Zotero/storage/JMJ9WPZ9/Shoeybi et al. - 2020 -
          Megatron-LM Training Multi-Billion Parameter Language Models Using
          Model Parallelism.pdf},
}

@misc{sniderOperatorFusionXLA2023,
  title = {Operator {{Fusion}} in {{XLA}}: {{Analysis}} and {{Evaluation}}},
  shorttitle = {Operator {{Fusion}} in {{XLA}}},
  author = {Snider, Daniel and Liang, Ruofan},
  year = {2023},
  month = jan,
  number = {arXiv:2301.13062},
  eprint = {2301.13062},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-20},
  abstract = {Machine learning (ML) compilers are an active area of research
              because they offer the potential to automatically speedup tensor
              programs. Kernel fusion is often cited as an important optimization
              performed by ML compilers. However, there exists a knowledge gap
              about how XLA, the most common ML compiler, applies this nuanced
              optimization, what kind of speedup it can afford, and what
              low-level effects it has on hardware. Our paper aims to bridge this
              knowledge gap by studying key compiler passes of XLA's source code.
              Our evaluation on a reinforcement learning environment Cartpole
              shows how different fusion decisions in XLA are made in practice.
              Furthermore, we implement several XLA kernel fusion strategies that
              can achieve up to 10.56x speedup compared to our baseline
              implementation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/kellen/Zotero/storage/9M5P2JQF/Snider and Liang - 2023 -
          Operator Fusion in XLA Analysis and Evaluation.pdf},
}

@misc{somerstepStatisticalFrameworkWeakstrong2024,
  title = {A Statistical Framework for Weak-to-Strong Generalization},
  author = {Somerstep, Seamus and Polo, Felipe Maia and Banerjee, Moulinath and
            Ritov, Ya'acov and Yurochkin, Mikhail and Sun, Yuekai},
  year = {2024},
  month = may,
  number = {arXiv:2405.16236},
  eprint = {2405.16236},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-09-25},
  abstract = {Modern large language model (LLM) alignment techniques rely on
              human feedback, but it is unclear whether the techniques
              fundamentally limit the capabilities of aligned LLMs. In particular
              , it is unclear whether it is possible to align (stronger) LLMs
              with superhuman capabilities with (weaker) human feedback without
              degrading their capabilities. This is an instance of the
              weak-to-strong generalization problem: using weaker (less capable)
              feedback to train a stronger (more capable) model. We prove that
              weak-to-strong generalization is possible by eliciting latent
              knowledge from pre-trained LLMs. In particular, we cast the
              weak-to-strong generalization problem as a transfer learning
              problem in which we wish to transfer a latent concept from a weak
              model to a strong pre-trained model. We prove that a naive
              fine-tuning approach suffers from fundamental limitations, but an
              alternative refinement-based approach suggested by the problem
              structure provably overcomes the limitations of fine-tuning.
              Finally, we demonstrate the practical applicability of the
              refinement approach in three LLM alignment tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/kellen/Zotero/storage/UIG2LRFF/Somerstep et al. - 2024 - A
          statistical framework for weak-to-strong general.pdf},
}

@misc{suttonHistoryMetagradientGradient2022,
  title = {A {{History}} of {{Meta-gradient}}: {{Gradient Methods}} for {{
           Meta-learning}}},
  shorttitle = {A {{History}} of {{Meta-gradient}}},
  author = {Sutton, Richard S.},
  year = {2022},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-09-06},
  abstract = {The history of meta-learning methods based on gradient descent is
              reviewed, focusing primarily on methods that adapt step-size
              (learning rate) meta-parameters.},
  howpublished = {https://arxiv.org/abs/2202.09701v1},
  langid = {english},
  file = {/home/kellen/Zotero/storage/93SUJHK4/Sutton - 2022 - A History of
          Meta-gradient Gradient Methods for Meta-learning.pdf},
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  abstract = {"Reinforcement learning, one of the most active research areas in
              artificial intelligence, is a computational approach to learning
              whereby an agent tries to maximize the total amount of reward it
              receives while interacting with a complex, uncertain environment.
              In Reinforcement Learning, Richard Sutton and Andrew Barto provide
              a clear and simple account of the field's key ideas and algorithms.
              "--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  lccn = {Q325.6 .R45 2018},
  keywords = {Reinforcement learning},
  file = {/home/kellen/Zotero/storage/DY4UI6G7/Sutton and Barto - 2018 -
          Reinforcement learning an introduction.pdf},
}

@misc{touatiDoesZeroShotReinforcement2023,
  title = {Does {{Zero-Shot Reinforcement Learning Exist}}?},
  author = {Touati, Ahmed and Rapin, J{\'e}r{\'e}my and Ollivier, Yann},
  year = {2023},
  month = mar,
  number = {arXiv:2209.14935},
  eprint = {2209.14935},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.14935},
  urldate = {2024-09-18},
  abstract = {A zero-shot RL agent is an agent that can solve any RL task in a
              given environment, instantly with no additional planning or
              learning, after an initial reward-free learning phase. This marks a
              shift from the reward-centric RL paradigm towards "controllable"
              agents that can follow arbitrary instructions in an environment.
              Current RL agents can solve families of related tasks at best, or
              require planning anew for each task. Strategies for approximate
              zero-shot RL ave been suggested using successor features (SFs)
              [BBQ+ 18] or forward-backward (FB) representations [TO21], but
              testing has been limited. After clarifying the relationships
              between these schemes, we introduce improved losses and new SF
              models, and test the viability of zero-shot RL schemes
              systematically on tasks from the Unsupervised RL benchmark
              [LYL+21]. To disentangle universal representation learning from
              exploration, we work in an offline setting and repeat the tests on
              several existing replay buffers. SFs appear to suffer from the
              choice of the elementary state features. SFs with Laplacian
              eigenfunctions do well, while SFs based on auto-encoders, inverse
              curiosity, transition models, low-rank transition matrix,
              contrastive learning, or diversity (APS), perform unconsistently.
              In contrast, FB representations jointly learn the elementary and
              successor features from a single, principled criterion. They
              perform best and consistently across the board, reaching 85\% of
              supervised RL performance with a good replay buffer, in a zero-shot
              manner.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/kellen/Zotero/storage/GFV8I99T/Touati et al. - 2023 - Does
          Zero-Shot Reinforcement Learning
          Exist.pdf;/home/kellen/Zotero/storage/9RA8LX73/2209.html},
}

@misc{touatiLearningOneRepresentation2021,
  title = {Learning {{One Representation}} to {{Optimize All Rewards}}},
  author = {Touati, Ahmed and Ollivier, Yann},
  year = {2021},
  month = oct,
  number = {arXiv:2103.07945},
  eprint = {2103.07945},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.07945},
  urldate = {2024-09-18},
  abstract = {We introduce the forward-backward (FB) representation of the
              dynamics of a reward-free Markov decision process. It provides
              explicit near-optimal policies for any reward specified a
              posteriori. During an unsupervised phase, we use reward-free
              interactions with the environment to learn two representations via
              off-the-shelf deep learning methods and temporal difference (TD)
              learning. In the test phase, a reward representation is estimated
              either from observations or an explicit reward description (e.g., a
              target state). The optimal policy for that reward is directly
              obtained from these representations, with no planning. We assume
              access to an exploration scheme or replay buffer for the first
              phase. The corresponding unsupervised loss is well-principled: if
              training is perfect, the policies obtained are provably optimal for
              any reward function. With imperfect training, the sub-optimality is
              proportional to the unsupervised approximation error. The FB
              representation learns long-range relationships between states and
              actions, via a predictive occupancy map, without having to
              synthesize states as in model-based approaches. This is a step
              towards learning controllable agents in arbitrary black-box
              stochastic environments. This approach compares well to
              goal-oriented RL algorithms on discrete and continuous mazes,
              pixel-based MsPacman, and the FetchReach virtual robot arm. We also
              illustrate how the agent can immediately adapt to new tasks beyond
              goal-oriented RL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Mathematics - Optimization and Control},
  file = {/home/kellen/Zotero/storage/FN8MPGES/Touati and Ollivier - 2021 -
          Learning One Representation to Optimize All
          Rewards.pdf;/home/kellen/Zotero/storage/SXVNRKWC/2103.html},
}

@misc{wangInvestigatingPropertiesNeural2023,
  title = {Investigating the {{Properties}} of {{Neural Network Representations}
           } in {{Reinforcement Learning}}},
  author = {Wang, Han and Miahi, Erfan and White, Martha and Machado, Marlos C.
            and Abbas, Zaheer and Kumaraswamy, Raksha and Liu, Vincent and White,
            Adam},
  year = {2023},
  month = may,
  number = {arXiv:2203.15955},
  eprint = {2203.15955},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-18},
  abstract = {In this paper we investigate the properties of representations
              learned by deep reinforcement learning systems. Much of the early
              work on representations for reinforcement learning focused on
              designing fixed-basis architectures to achieve properties thought
              to be desirable, such as orthogonality and sparsity. In contrast,
              the idea behind deep reinforcement learning methods is that the
              agent designer should not encode representational properties, but
              rather that the data stream should determine the properties of the
              representation---good representations emerge under appropriate
              training schemes. In this paper we bring these two perspectives
              together, empirically investigating the properties of
              representations that support transfer in reinforcement learning. We
              introduce and measure six representational properties over more
              than 25 thousand agent-task settings. We consider Deep Q-learning
              agents with different auxiliary losses in a pixel-based navigation
              environment, with source and transfer tasks corresponding to
              different goal locations. We develop a method to better understand
              why some representations work better for transfer, through a
              systematic approach varying task similarity and measuring and
              correlating representation properties with transfer performance. We
              demonstrate the generality of the methodology by investigating
              representations learned by a Rainbow agent that successfully
              transfer across games modes in Atari 2600.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/kellen/Zotero/storage/3QBBT8HR/Wang et al. - 2023 -
          Investigating the Properties of Neural Network Representations in
          Reinforcement Learning.pdf},
}

@misc{xieExplanationContextLearning2022,
  title = {An {{Explanation}} of {{In-context Learning}} as {{Implicit Bayesian
           Inference}}},
  author = {Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma,
            Tengyu},
  year = {2022},
  month = jul,
  number = {arXiv:2111.02080},
  eprint = {2111.02080},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-15},
  abstract = {Large language models (LMs) such as GPT-3 have the surprising
              ability to do in-context learning, where the model learns to do a
              downstream task simply by conditioning on a prompt consisting of
              input-output examples. The LM learns from these examples without
              being explicitly pretrained to learn. Thus, it is unclear what
              enables in-context learning. In this paper, we study how in-context
              learning can emerge when pretraining documents have long-range
              coherence. Here, the LM must infer a latent document-level concept
              to generate coherent next tokens during pretraining. At test time,
              in-context learning occurs when the LM also infers a shared latent
              concept between examples in a prompt. We prove when this occurs
              despite a distribution mismatch between prompts and pretraining
              data in a setting where the pretraining distribution is a mixture
              of HMMs. In contrast to messy large-scale datasets used to train
              LMs capable of in-context learning, we generate a small-scale
              synthetic dataset (GINC) where Transformers and LSTMs both exhibit
              in-context learning1. Beyond the theory, experiments on GINC
              exhibit large-scale real-world phenomena including improved
              in-context performance with model scaling (despite the same
              pretraining loss), sensitivity to example order, and instances
              where zero-shot is better than few-shot in-context learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science -
              Machine Learning},
  file = {/home/kellen/Zotero/storage/RKQR98C3/Xie et al. - 2022 - An
          Explanation of In-context Learning as Implicit Bayesian Inference.pdf},
}

@inproceedings{yangRepresentationMattersOffline2021,
  title = {Representation {{Matters}}: {{Offline Pretraining}} for {{Sequential
           Decision Making}}},
  shorttitle = {Representation {{Matters}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine
               Learning}}},
  author = {Yang, Mengjiao and Nachum, Ofir},
  year = {2021},
  month = jul,
  pages = {11784--11794},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-09-08},
  abstract = {The recent success of supervised learning methods on ever larger
              offline datasets has spurred interest in the reinforcement learning
              (RL) field to investigate whether the same paradigms can be
              translated to RL algorithms. This research area, known as offline
              RL, has largely focused on offline policy optimization, aiming to
              find a return-maximizing policy exclusively from offline data. In
              this paper, we consider a slightly different approach to
              incorporating offline data into sequential decision-making. We aim
              to answer the question, what unsupervised objectives applied to
              offline datasets are able to learn state representations which
              elevate performance on downstream tasks, whether those downstream
              tasks be online RL, imitation learning from expert demonstrations,
              or even offline policy optimization based on the same offline
              dataset? Through a variety of experiments utilizing standard
              offline RL datasets, we find that the use of pretraining with
              unsupervised learning objectives can dramatically improve the
              performance of policy learning algorithms that otherwise yield
              mediocre performance on their own. Extensive ablations further
              provide insights into what components of these unsupervised
              objectives \{--\} e.g., reward prediction, continuous or discrete
              representations, pretraining or finetuning \{--\} are most
              important and in which settings.},
  langid = {english},
  file = {/home/kellen/Zotero/storage/B54IUCU7/Yang and Nachum - 2021 -
          Representation Matters Offline Pretraining for Sequential Decision
          Making.pdf;/home/kellen/Zotero/storage/BANYUMGT/Yang and Nachum - 2021
          - Representation Matters Offline Pretraining for Sequential Decision
          Making.pdf},
}

@misc{yeOnlineIterativeReinforcement2024,
  title = {Online {{Iterative Reinforcement Learning}} from {{Human Feedback}}
           with {{General Preference Model}}},
  author = {Ye, Chenlu and Xiong, Wei and Zhang, Yuheng and Jiang, Nan and Zhang
            , Tong},
  year = {2024},
  month = apr,
  number = {arXiv:2402.07314},
  eprint = {2402.07314},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-09-16},
  abstract = {We study Reinforcement Learning from Human Feedback (RLHF) under a
              general preference oracle. In particular, we do not assume that
              there exists a reward function and the preference signal is drawn
              from the Bradley-Terry model as most of the prior works do. We
              consider a standard mathematical formulation, the reverse-KL
              regularized minimax game between two LLMs for RLHF under general
              preference oracle. The learning objective of this formulation is to
              find a policy so that it is consistently preferred by the
              KL-regularized preference oracle over any competing LLMs. We show
              that this framework is strictly more general than the reward-based
              one, and propose sample-efficient algorithms for both the offline
              learning from a pre-collected preference dataset and online
              learning where we can query the preference oracle along the way of
              training. Empirical studies verify the effectiveness of the
              proposed framework.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/kellen/Zotero/storage/RG97WJKY/Ye et al. - 2024 - Online
          Iterative Reinforcement Learning from Human Feedback with General
          Preference Model.pdf},
}

@misc{zhangRevisitingZerothOrderOptimization2024,
  title = {Revisiting {{Zeroth-Order Optimization}} for {{Memory-Efficient LLM
           Fine-Tuning}}: {{A Benchmark}}},
  shorttitle = {Revisiting {{Zeroth-Order Optimization}} for {{Memory-Efficient
                LLM Fine-Tuning}}},
  author = {Zhang, Yihua and Li, Pingzhi and Hong, Junyuan and Li, Jiaxiang and
            Zhang, Yimeng and Zheng, Wenqing and Chen, Pin-Yu and Lee, Jason D.
            and Yin, Wotao and Hong, Mingyi and Wang, Zhangyang and Liu, Sijia
            and Chen, Tianlong},
  year = {2024},
  month = may,
  number = {arXiv:2402.11592},
  eprint = {2402.11592},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.11592},
  urldate = {2024-09-08},
  abstract = {In the evolving landscape of natural language processing (NLP),
              fine-tuning pre-trained Large Language Models (LLMs) with
              first-order (FO) optimizers like SGD and Adam has become standard.
              Yet, as LLMs grow \{in size\}, the substantial memory overhead from
              back-propagation (BP) for FO gradient computation presents a
              significant challenge. Addressing this issue is crucial, especially
              for applications like on-device training where memory efficiency is
              paramount. This paper proposes a shift towards BP-free,
              zeroth-order (ZO) optimization as a solution for reducing memory
              costs during LLM fine-tuning, building on the initial concept
              introduced by MeZO. Unlike traditional ZO-SGD methods, our work
              expands the exploration to a wider array of ZO optimization
              techniques, through a comprehensive, first-of-its-kind benchmarking
              study across five LLM families (Roberta, OPT, LLaMA, Vicuna,
              Mistral), three task complexities, and five fine-tuning schemes.
              Our study unveils previously overlooked optimization principles,
              highlighting the importance of task alignment, the role of the
              forward gradient method, and the balance between algorithm
              complexity and fine-tuning performance. We further introduce novel
              enhancements to ZO optimization, including block-wise descent,
              hybrid training, and gradient sparsity. Our study offers a
              promising direction for achieving further memory-efficient LLM
              fine-tuning. Codes to reproduce all our experiments are at
              https://github.com/ZO-Bench/ZO-LLM .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science -
              Machine Learning},
  file = {/home/kellen/Zotero/storage/3FC8QZYA/Zhang et al. - 2024 - Revisiting
          Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning A
          Benchmark.pdf},
}

@misc{zhaoPyTorchFSDPExperiences2023,
  title = {{{PyTorch FSDP}}: {{Experiences}} on {{Scaling Fully Sharded Data
           Parallel}}},
  shorttitle = {{{PyTorch FSDP}}},
  author = {Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang
            , Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and
            Ott, Myle and Shleifer, Sam and Desmaison, Alban and Balioglu, Can
            and Damania, Pritam and Nguyen, Bernard and Chauhan, Geeta and Hao,
            Yuchen and Mathews, Ajit and Li, Shen},
  year = {2023},
  month = apr,
  journal = {arXiv.org},
  urldate = {2024-09-06},
  abstract = {It is widely acknowledged that large models have the potential to
              deliver superior performance across a broad range of domains.
              Despite the remarkable progress made in the field of machine
              learning systems research, which has enabled the development and
              exploration of large models, such abilities remain confined to a
              small group of advanced users and industry leaders, resulting in an
              implicit technical barrier for the wider community to access and
              leverage these technologies. In this paper, we introduce PyTorch
              Fully Sharded Data Parallel (FSDP) as an industry-grade solution
              for large model training. FSDP has been closely co-designed with
              several key PyTorch core components including Tensor implementation
              , dispatcher system, and CUDA memory caching allocator, to provide
              non-intrusive user experiences and high training efficiency.
              Additionally, FSDP natively incorporates a range of techniques and
              settings to optimize resource utilization across a variety of
              hardware configurations. The experimental results demonstrate that
              FSDP is capable of achieving comparable performance to Distributed
              Data Parallel while providing support for significantly larger
              models with near-linear scalability in terms of TFLOPS.},
  howpublished = {https://arxiv.org/abs/2304.11277v2},
  langid = {english},
  file = {/home/kellen/Zotero/storage/C7SNWUS4/Zhao et al. - 2023 - PyTorch FSDP
          Experiences on Scaling Fully Sharded Data Parallel.pdf},
}

@misc{zhengContrastiveDifferencePredictive2024,
  title = {Contrastive {{Difference Predictive Coding}}},
  author = {Zheng, Chongyi and Salakhutdinov, Ruslan and Eysenbach, Benjamin},
  year = {2024},
  month = feb,
  number = {arXiv:2310.20141},
  eprint = {2310.20141},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.20141},
  urldate = {2024-09-06},
  abstract = {Predicting and reasoning about the future lie at the heart of many
              time-series questions. For example, goal-conditioned reinforcement
              learning can be viewed as learning representations to predict which
              states are likely to be visited in the future. While prior methods
              have used contrastive predictive coding to model time series data,
              learning representations that encode long-term dependencies usually
              requires large amounts of data. In this paper, we introduce a
              temporal difference version of contrastive predictive coding that
              stitches together pieces of different time series data to decrease
              the amount of data required to learn predictions of future events.
              We apply this representation learning method to derive an
              off-policy algorithm for goal-conditioned RL. Experiments
              demonstrate that, compared with prior RL methods, ours achieves \$2
              {\textbackslash}times\$ median improvement in success rates and can
              better cope with stochastic environments. In tabular settings, we
              show that our method is about \$20 {\textbackslash}times\$ more
              sample efficient than the successor representation and \$1500 {
              \textbackslash}times\$ more sample efficient than the standard
              (Monte Carlo) version of contrastive predictive coding.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning},
  file = {/home/kellen/Zotero/storage/LXYBRXB5/Zheng et al. - 2024 - Contrastive
          Difference Predictive Coding.pdf},
}
