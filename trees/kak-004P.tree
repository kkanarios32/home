\date{2025-02-03}
\title{Convergence of accelerated GD}
\taxon{theorem}
\p{
Suppose #{f:\mathbb{R}^{n}\mapsto\mathbb{R}} is convex and smooth with its gradient #{L}-Lipschitz. The iterates #{\{\mathbf{x}_{k}\}_{k\geq1}} generated by the accelerated GD method satisfy

##{f(\mathbf{x}_{k})-f(\mathbf{x}_{\star})\ \leq\ \frac{L}{2(k+1)^{2}}\ ||\mathbf{x}_{0}-\mathbf{x}_{\star}||_{2}^{2}\ =\ O(\frac{1}{k^{2}}).}

Moreover, as #{k\to+\infty}, then #{\mathbf{x}_{k}\to\mathbf{x}_{\star}}.
}
