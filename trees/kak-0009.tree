\date{2024-10-31}
\title{Contrastive Learning}
\import{latex-preamble}
\import{base-macros}
\p{Prior to understanding contrastive reinforcement learning, it is important to have an at least rudimentary understanding of contrastive learning. Historically, contrastive learning has been used to learn representations. The fundamental idea behind contrastive learning is to encourage the representations of similar outputs to be similar in representation space.}
\p{\strong{Supervised setting:} For now, assume we are in the supervised setting (we have access to lables). Suppose that we are learning a representation in #{\mathbb{R}^d}. Our model is a classifier on dogs and cats. If we have two dogs #{y_1} and #{y_2} then we want the learned representation map ##{\phi: \{\text{dogs}, \text{cats}\} \to \mathbb{R}^d} to be such that #{\phi(y_1)} and #{\phi(y_2)} are "close" in #{\mathbb{R}^d}. Now the notion of "close" is to be determined by the user. An example could be to minimize the inner product between their representation maps i.e. we could learn a feature map parametrized by #{\theta} with the following objective ##{\max_{\theta}\ \langle \phi_{\theta}(y_1), \phi_{\theta}(y_2) \rangle.} Similarly, we want dissimilar outputs to be far apart in representation space. If #{y_3} is a cat, then we can introduce a regularization to encourage this i.e.
##{\max_{\theta}\ \langle \phi_{\theta}(y_1), \phi_{\theta}(y_2) \rangle - \sum_{i \in \{1, 2\}} \langle \phi_{\theta}(y_i), \phi_{\theta}(y_3) \rangle.} The astute reader will (correctly) ask why we would do this over supervised learning? The answer is we wouldn't. This is just an illustrative example. The real power of contrastive learning comes from being able to utilize un-labeled samples.}
\p{ \strong{Unsupervised setting:} Now suppose that we get rid of labels and are just given #{n} dog samples #{\mathcal{D}} from some distribution #{p_{\mathcal{D}}}. We now want to be able to learn #{p_{\theta}} to somehow estimate this distribution. An approach is to learn to distinguish the sample dogs given from random noise. To do so, we generate #{n} random images #{\mathcal{R}} according to some distribution #{p_{\mathcal{R}}}. We can now return to the supervised learning setting, where we treat #{\mathcal{D}} and #{\mathcal{R}} as two classes. If we recall standard supervised learning practice, given a sample #{x}, we then want to find ##{p(\mathcal{D} \mid x) = 1 - p(\mathcal{R} \mid X).} 
As an explicit example, we will use logistic regression. Namely, we will model #{p(x) = p(\mathcal{D} \mid x)} as ##{p_{\theta}(x) = \frac{1}{1 + e^{-G_{\theta}(x)}}.} However, #{p_{\theta}(x)} is estimating #{p(\mathcal{D} \mid x)}, where we care about #{p(x \mid \mathcal{D})}. To estimate the correct quantity, we need to leverage our knowledge of the noise distribution. Recall that if #{p_{\theta}(x) = p(\mathcal{D} \mid x)} then #{G_{\theta}(x) = \log \frac{p(x \mid \mathcal{D})}{p(x \mid \mathcal{R})}}. Since we generated the samples from #{\mathcal{R}}, we have the explicit distribution i.e. #{p(x \mid \mathcal{R}) = p_{\mathcal{R}}(x)}. Therefore, we can restrict #{G_{\theta}} to explicitly learn #{p(x \mid \mathcal{D})} by considering ##{G_{\theta}(x) = \log p_{\theta}(x \mid \mathcal{D}) - \log p_{\mathcal{R}}(x),} considering the cross entropy loss we get the [NCE loss](nce)
}
\transclude{kak-000E}

\p{In [[gutmann2012]], they show under mild conditions that the estimator #{p_{\theta}(x \mid D) \to p_{\mathcal{D}}(x)} in probability as the number of samples in the loss goes to infinity. Equivalently, the estimator is [consistent](kak-000F).}

\p{\strong{Time series:} Before we get to contrastive RL, it is a natural question to wonder how does this apply to temporal sequences? Concretely, we want to make predictions about the future given the current "context". However, we want to do so in an unsupervised way, meaning we are only given trajectories not a notion of what it means for a trajectory to be good. Naively, one can try to do this in a supervised manner. For a #{k} step prediction, this would just be your model predicting what will happen in #{k} steps then seeing if it matches what occured #{k} steps in the future in the sample trajectory. However, if your sample space #{\mathcal{X}} is very high-dimensional, modeling this relationship can require an exorbinant amount of trajectories.}

\p{Fast forwarding to contrastive RL, current work is primarily considered with a particular contrastive objective.}

\transclude{kak-000B}

\p{Now we need to unpack this very ominous loss. To start, what are #{x_k} and #{c_t}?}

\transclude{kak-000C}
