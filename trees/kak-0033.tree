\date{2025-01-26}
\import{base-macros}
\tag{lost}
\tag{note}
\tag{unfinished}
\title{Optimization Theory}
\author{kellenkanarios}


\section{Convex Optimization Background}{

\transclude{kak-0034}
\transclude{kak-0035}

\p{
\strong{Operations that preserve convexity}
\ul{
    \li{\strong{Nonnegative weighted sums:} if #{f_1, \ldots, f_n} are convex and #{\alpha_1, \ldots, \alpha_n > 0} then we have #{f(\boldsymbol{x}) = \sum_{i = 1}^{n} \alpha_i f_i(\boldsymbol{x})} is convex}
    \li{\strong{Pointwise maximum:} if #{f_1, \ldots, f_n} are convex then #{f(\boldsymbol{x}) = \max_i \{f_i(\boldsymbol{x})\}} is convex }
  }
}

\transclude{kak-0036}
\transclude{kak-0037}

\transclude{kak-0038}
\transclude{kak-0039}
}

\section{Useful Miscellanous Math Background}{
  \p{
  In this section, we cover many random math facts / tools that come up / are useful in later optimization problems.
  }
    \section{Basic Matrix Analysis}{
\ul{
    \li{\strong{Matrix inner product:}
##{\langle\mathbf{x},\mathbf{z}\rangle\ =\ \sum_{i=1}^{n}x_{i}z_{i}\quad\Longrightarrow\quad\langle\mathbf{X},\mathbf{Z}\rangle\ :=\ \sum_{i=1}^{m}\sum_{j=1}^{n}X_{i j}Z_{i j}} 
    }
    \li{\strong{Matrix trace:} #{{{M}}\in\mathbb{R}^{n\times n};\ {{\mathrm{tr}}}({{M}}):=\sum_{i=1}^{n}M_{i i}}
    ##{
      \begin{align*}
      &\langle{\boldsymbol X},{\boldsymbol Z}\rangle\ =\ \mathrm{tr}\left({\boldsymbol X}^{\top}{\boldsymbol Z}\right)\ =\ \mathrm{tr}\left({\boldsymbol X}{\boldsymbol Z}^{\top}\right) \\
    &\operatorname{tr}(\mathbf{A}\mathbf{B})=\operatorname{tr}(\mathbf{B}\mathbf{A})\\
      &\operatorname{tr}(\mathbf{A}_{1}\mathbf{A}_{2}\cdots\mathbf{A}_{n})=\operatorname{tr}(\mathbf{A}_{\pi(1)}\mathbf{A}_{\pi(2)}\cdots\mathbf{A}_{\pi(n)})
      \end{align*}
    }
    Where #{\pi} is a cyclic permutation on #{\{1,2,\cdots,n\}}
    } 
    }
    \transclude{kak-003A}
    \transclude{kak-003B}
    \transclude{kak-003C}
    \transclude{kak-003G}
    \transclude{kak-003H}
    \transclude{kak-003I}
  }
    \section{Taylor Expansion and Lipschitz Functions}{
      \transclude{kak-003J}
      \transclude{kak-003K}
      \transclude{kak-003L}
      \remark{
        \ul{
          \li{ If #{f:\mathbb{R}^n\mapsto\mathbb{R}} is continuously differentiable, ##{\begin{aligned}      |f(\pmb{x})-f(\pmb{y})| &\leq & \sup_{\pmb{z} \in \mathbb{R}^n} ||\nabla f(\pmb{z})||_2 \, ||\pmb{x}-\pmb{y}||_2   \end{aligned}} so that the Lipschitz constant of #{f} is #{\sup_{\pmb{z} \in \mathbb{R}^n} ||\nabla f(\pmb{z})||_2}.}
          \li{If #{f:\mathbb{R}^{n}\mapsto\mathbb{R}} is twice continuously differentiable, ##{||\nabla f(\boldsymbol{x})-\nabla f(\boldsymbol{y})||_{2}\ \leq\ \sup_{\boldsymbol{z}\in\mathbb{R}^{n}}||\nabla^{2}f(\boldsymbol{z})||_{2}\,||\boldsymbol{x}-\boldsymbol{y}||_{2}} so that the Lipschitz constant of #{\nabla f} is #{\sup_{\boldsymbol{z}\in\mathbb{R}^{n}}||\nabla^{2}f(\boldsymbol{z})||_{2}}
          }
        }
      }
      \transclude{kak-003M}
      \transclude{kak-003N}
      \transclude{kak-003O}
    }
  \section{Optimality Conditions}{
    \transclude{kak-003P}
    \ul{
    \li{\strong{First order necessary condition}:
    If #{\ \mathbf{x_{\star}}} is a local minimizer of #{\ f:\mathbb{R}^{n}\mapsto\mathbb{R}}, and #{f} is continuously differentiable in an open neighborhood of #{\ \mathbf{x_{\star}}}, then we must have ##{\boxed{\nabla f(\mathbf{x_{\star}})\ =\ \mathbf{0}.}}
    \li{\strong{Second order necessary condition:}
    If #{\mathbf{x_{\star}}} is a local minimizer of #{f:\mathbb{R}^{n}\mapsto\mathbb{R}}, and #{f} is twice continuously differentiable in an open neighborhood around #{\mathbf{x_{\star}}}, then we must have ##{(i)\ \nabla f(\mathbf{x_{\star}})=\mathbf{0};} ##{(ii)\ \nabla^{2}f(\mathbf{x_{\star}})\succeq\mathbf{0}.}
    \ul{
        \li{If #{\nabla^{2}f(\mathbf{x_{\star}})\succ\mathbf{0}} then the above is a sufficient condition.}
      }
    }
    }
  }
  \remark{
  Let #{f:\mathbb{R}^{n}\mapsto\mathbb{R}} be a convex function, then 
  \ul{
    \li{A local minimizer of #{f} is also its global minimizer. If #{f} is strictly convex, the global minimizer is unique}
    \li{A point is a global minimizer of #{f} iff ##{\mathbf{0}\ \in\ \partial f(\mathbf{x_{\star}}).}}
  }
  If #{f\in\mathcal{C}^{1}}, then #{\nabla f(\mathbf{x_{\star}})=\mathbf{0}} implies that #{\mathbf{x_{\star}}} is a global minimizer.
  }
  \section{Constrained Case}{
    Consider a smooth contained problem with ##{\min_{\mathbf{x}}f(\mathbf{x}),\quad \text{s.t.} \quad r_{i}(\mathbf{x})\ =\ 0,\ 1\leq i\leq p,} ##{h_{j}(\mathbf{x})\ \leq\ 0,\ 1\leq j\leq q.} Consider its Lagrangian function ##{\mathscr{L}(\mathbf{x},\mathbf{u},\mathbf{v})\ =\ f(\mathbf{x})+\sum_{i=1}^{p}u_{i}\cdot r_{i}(\mathbf{x})+\sum_{j=1}^{q}v_{j}\cdot h_{j}(\mathbf{x}).}
    \strong{KKT-conditions:}
    \ul{
    \li{Stationary ##{\nabla_{\mathbf{x}}{\mathcal{L}}(\mathbf{x_*},\mathbf{u_*},\mathbf{v_*})\;=\;\mathbf{0}}}
    \li{Primal Feasibility: ##{\begin{array}{l}{{r_{i}(x_{\star})\ =\ 0,\ 1\leq i\leq p,}}\\ {{h_{j}(x_{\star})\ \leq\ 0,\ 1\leq j\leq q.}}\end{array}}}
    \li{Dual Feasibility: ##{v_{\star}\geq 0.}}
    \li{Complimentary slackness: ##{h_{j}(x_{\star})\cdot v_{\star j}\ =\ 0,\quad\forall\ 1\leq j\leq q.}}
    }
  }
  }
  \section{Rate of Convergence}{
    \p{
    Want to find
##{\operatorname*{min}_{\mathbf{x}}\ f(\mathbf{x}),\quad\text{s.t.}\quad\mathbf{x}\ \in\ \mathcal{C}.} Solve the problem via iterative methods of optimization, which produce a sequence of points ##{\mathbf{x}_{1},\ \mathbf{x}_{2},\ \cdots,\mathbf{x}_{k},\cdots} starting from an initialization #{\mathbf{x}_{0}}. 
    }
    \transclude{kak-003R}
    \transclude{kak-003Q}
    \transclude{kak-003S}
  }
}
