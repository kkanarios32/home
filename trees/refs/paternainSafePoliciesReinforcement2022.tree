% ["references"]
\title{Safe Policies for Reinforcement Learning via Primal-Dual Methods}
\date{2022-01}
\author{Santiago Paternain}\author{Miguel Calvo-Fullana}\author{Luiz F. O. Chamon}\author{Alejandro Ribeiro}
\taxon{reference}
\meta{doi}{10.48550/arXiv.1911.09101}
\meta{external}{https://arxiv.org/abs/1911.09101}

\meta{bibtex}{\startverb
@misc{paternainSafePoliciesReinforcement2022,
 title = {Safe {{Policies}} for {{Reinforcement Learning}} via {{Primal-Dual Methods}}},
 author = {Paternain, Santiago and {Calvo-Fullana}, Miguel and Chamon, Luiz F. O. and Ribeiro, Alejandro},
 year = {2022},
 doi = {10.48550/arXiv.1911.09101},
 urldate = {2025-01-27},
 number = {arXiv:1911.09101},
 publisher = {arXiv},
 file = {/home/kellen/Zotero/storage/F3LLEA4F/Paternain et al. - 2022 - Safe Policies for Reinforcement Learning via Primal-Dual Methods.pdf},
 keywords = {Computer Science - Machine Learning,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {In this paper, we study the design of controllers in the context of stochastic optimal control under the assumption that the model of the system is not available. This is, we aim to control a Markov Decision Process (MDP) of which we do not know the transition probabilities, but we have access to sample trajectories through experience. We define safety as the agent remaining in a desired safe set with high probability during the operation time. The drawbacks of this formulation are twofold. The problem is non-convex and computing the gradients of the constraints with respect to the policies is prohibitive. Hence, we propose an ergodic relaxation of the constraints with the following advantages. (i) The safety guarantees are maintained in the case of episodic tasks and they hold until a given time horizon for continuing tasks. (ii) The constrained optimization problem despite its non-convexity has arbitrarily small duality gap if the parametrization of the controller is rich enough. (iii) The gradients of the Lagrangian associated to the safe learning problem can be computed using standard Reinforcement Learning (RL) results and stochastic approximation tools. Leveraging these advantages, we exploit primal-dual algorithms to find policies that are safe and optimal. We test the proposed approach in a navigation task in a continuous domain. The numerical results show that our algorithm is capable of dynamically adapting the policy to the environment and the required safety levels.},
 primaryclass = {eess},
 eprint = {1911.09101},
 month = {January}
}
\stopverb}