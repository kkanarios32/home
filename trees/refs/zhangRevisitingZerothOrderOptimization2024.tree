% ["references"]
\title{Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark}
\date{2024-05}
\author{Yihua Zhang}\author{Pingzhi Li}\author{Junyuan Hong}\author{Jiaxiang Li}\author{Yimeng Zhang}\author{Wenqing Zheng}\author{Pin-Yu Chen}\author{Jason D. Lee}\author{Wotao Yin}\author{Mingyi Hong}\author{Zhangyang Wang}\author{Sijia Liu}\author{Tianlong Chen}
\taxon{reference}
\meta{doi}{10.48550/arXiv.2402.11592}
\meta{external}{https://arxiv.org/abs/2402.11592}

\meta{bibtex}{\startverb
@misc{zhangRevisitingZerothOrderOptimization2024,
 title = {Revisiting {{Zeroth-Order Optimization}} for {{Memory-Efficient LLM Fine-Tuning}}: {{A Benchmark}}},
 author = {Zhang, Yihua and Li, Pingzhi and Hong, Junyuan and Li, Jiaxiang and Zhang, Yimeng and Zheng, Wenqing and Chen, Pin-Yu and Lee, Jason D. and Yin, Wotao and Hong, Mingyi and Wang, Zhangyang and Liu, Sijia and Chen, Tianlong},
 year = {2024},
 doi = {10.48550/arXiv.2402.11592},
 urldate = {2024-09-08},
 number = {arXiv:2402.11592},
 publisher = {arXiv},
 file = {/home/kellen/Zotero/storage/3FC8QZYA/Zhang et al. - 2024 - Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning A Benchmark.pdf},
 keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
 archiveprefix = {arXiv},
 abstract = {In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow \{in size\}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM .},
 primaryclass = {cs},
 eprint = {2402.11592},
 month = {May},
 shorttitle = {Revisiting {{Zeroth-Order Optimization}} for {{Memory-Efficient LLM Fine-Tuning}}}
}
\stopverb}