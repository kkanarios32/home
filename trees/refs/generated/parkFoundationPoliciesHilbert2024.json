[{"DOI": "10.48550/arXiv.2402.15567", "URL": "https://arxiv.org/abs/2402.15567", "abstract": "Unsupervised and self-supervised objectives, such as next token prediction, have enabled pre-training generalist models from large amounts of unlabeled data. In reinforcement learning (RL), however, finding a truly general and scalable unsupervised pre-training objective for generalist policies from offline data remains a major open question. While a number of methods have been proposed to enable generic self-supervised RL, based on principles such as goal-conditioned RL, behavioral cloning, and unsupervised skill learning, such methods remain limited in terms of either the diversity of the discovered behaviors, the need for high-quality demonstration data, or the lack of a clear adaptation mechanism for downstream tasks. In this work, we propose a novel unsupervised framework to pre-train generalist policies that capture diverse, optimal, long-horizon behaviors from unlabeled offline data such that they can be quickly adapted to any arbitrary new tasks in a zero-shot manner. Our key insight is to learn a structured representation that preserves the temporal structure of the underlying environment, and then to span this learned latent space with directional movements, which enables various zero-shot policy \"prompting\" schemes for downstream tasks. Through our experiments on simulated robotic locomotion and manipulation benchmarks, we show that our unsupervised policies can solve goal-conditioned and general RL tasks in a zero-shot fashion, even often outperforming prior methods designed specifically for each setting. Our code and videos are available at https://seohong.me/projects/hilp/.", "accessed": {"date-parts": [[2024, 9, 18]]}, "author": [{"family": "Park", "given": "Seohong"}, {"family": "Kreiman", "given": "Tobias"}, {"family": "Levine", "given": "Sergey"}], "id": "parkFoundationPoliciesHilbert2024", "issued": {"date-parts": [[2024, 5]]}, "keyword": "Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics", "number": "arXiv:2402.15567", "publisher": "arXiv", "title": "Foundation Policies with Hilbert Representations", "type": "", "original_bibtex": "@misc{parkFoundationPoliciesHilbert2024,\n title = {Foundation {{Policies}} with {{Hilbert Representations}}},\n author = {Park, Seohong and Kreiman, Tobias and Levine, Sergey},\n year = {2024},\n doi = {10.48550/arXiv.2402.15567},\n urldate = {2024-09-18},\n number = {arXiv:2402.15567},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/WEHZ63LJ/Park et al. - 2024 - Foundation Policies with Hilbert Representations.pdf;/home/kellen/Zotero/storage/YNKYPBF3/Park et al. - 2024 - Foundation Policies with Hilbert Representations.pdf;/home/kellen/Zotero/storage/5H2B6U6Y/2402.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},\n archiveprefix = {arXiv},\n abstract = {Unsupervised and self-supervised objectives, such as next token prediction, have enabled pre-training generalist models from large amounts of unlabeled data. In reinforcement learning (RL), however, finding a truly general and scalable unsupervised pre-training objective for generalist policies from offline data remains a major open question. While a number of methods have been proposed to enable generic self-supervised RL, based on principles such as goal-conditioned RL, behavioral cloning, and unsupervised skill learning, such methods remain limited in terms of either the diversity of the discovered behaviors, the need for high-quality demonstration data, or the lack of a clear adaptation mechanism for downstream tasks. In this work, we propose a novel unsupervised framework to pre-train generalist policies that capture diverse, optimal, long-horizon behaviors from unlabeled offline data such that they can be quickly adapted to any arbitrary new tasks in a zero-shot manner. Our key insight is to learn a structured representation that preserves the temporal structure of the underlying environment, and then to span this learned latent space with directional movements, which enables various zero-shot policy \"prompting\" schemes for downstream tasks. Through our experiments on simulated robotic locomotion and manipulation benchmarks, we show that our unsupervised policies can solve goal-conditioned and general RL tasks in a zero-shot fashion, even often outperforming prior methods designed specifically for each setting. Our code and videos are available at https://seohong.me/projects/hilp/.},\n primaryclass = {cs},\n eprint = {2402.15567},\n month = {May}\n}\n"}]