[{"DOI": "10.48550/arXiv.2402.17135", "URL": "https://arxiv.org/abs/2402.17135", "abstract": "Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a functional reward encoding (FRE) as a general, scalable solution to this zero-shot RL problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformerbased variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zeroshot manner, given a small number of rewardannotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL and offline RL methods. Code for this project is provided at: github.com/kvfrans/fre.", "accessed": {"date-parts": [[2024, 12, 4]]}, "author": [{"family": "Frans", "given": "Kevin"}, {"family": "Park", "given": "Seohong"}, {"family": "Abbeel", "given": "Pieter"}, {"family": "Levine", "given": "Sergey"}], "id": "fransUnsupervisedZeroShotReinforcement2024", "issued": {"date-parts": [[2024, 2]]}, "keyword": "Computer Science - Artificial Intelligence,Computer Science - Machine Learning", "language": "en-US", "number": "arXiv:2402.17135", "publisher": "arXiv", "title": "Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings", "type": "", "original_bibtex": "@misc{fransUnsupervisedZeroShotReinforcement2024,\n title = {Unsupervised {{Zero-Shot Reinforcement Learning}} via {{Functional Reward Encodings}}},\n author = {Frans, Kevin and Park, Seohong and Abbeel, Pieter and Levine, Sergey},\n year = {2024},\n doi = {10.48550/arXiv.2402.17135},\n urldate = {2024-12-04},\n number = {arXiv:2402.17135},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/FUDVV3U4/Frans et al. - 2024 - Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a functional reward encoding (FRE) as a general, scalable solution to this zero-shot RL problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformerbased variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zeroshot manner, given a small number of rewardannotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL and offline RL methods. Code for this project is provided at: github.com/kvfrans/fre.},\n primaryclass = {cs},\n eprint = {2402.17135},\n month = {February}\n}\n"}]