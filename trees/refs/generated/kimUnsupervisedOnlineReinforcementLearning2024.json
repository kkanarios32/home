[{"abstract": "Offline-to-online reinforcement learning (RL), a framework that trains a policy with offline RL and then further fine-tunes it with online RL, has been considered a promising recipe for data-driven decision-making. While sensible, this framework has drawbacks: it requires domain-specific offline RL pre-training for each task, and is often brittle in practice. In this work, we propose unsupervised-to-online RL (U2O RL), which replaces domain-specific supervised offline RL with unsupervised offline RL, as a better alternative to offline-to-online RL. U2O RL not only enables reusing a single pre-trained model for multiple downstream tasks, but also learns better representations, which often result in even better performance and stability than supervised offline-to-online RL. To instantiate U2O RL in practice, we propose a general recipe for U2O RL to bridge task-agnostic unsupervised offline skill-based policy pre-training and supervised online fine-tuning. Throughout our experiments in nine state-based and pixel-based environments, we empirically demonstrate that U2O RL achieves strong performance that matches or even outperforms previous offline-to-online RL approaches, while being able to reuse a single pre-trained model for a number of different downstream tasks.", "accessed": {"date-parts": [[2024, 9, 6]]}, "author": [{"family": "Kim", "given": "Junsu"}, {"family": "Park", "given": "Seohong"}, {"family": "Levine", "given": "Sergey"}], "container-title": "arXiv.org", "id": "kimUnsupervisedOnlineReinforcementLearning2024", "issued": {"date-parts": [[2024, 8]]}, "language": "en-US", "publisher": "https://arxiv.org/abs/2408.14785v1", "title": "Unsupervised-to-Online Reinforcement Learning", "type": "", "original_bibtex": "@misc{kimUnsupervisedOnlineReinforcementLearning2024,\n title = {Unsupervised-to-{{Online Reinforcement Learning}}},\n author = {Kim, Junsu and Park, Seohong and Levine, Sergey},\n year = {2024},\n urldate = {2024-09-06},\n howpublished = {https://arxiv.org/abs/2408.14785v1},\n journal = {arXiv.org},\n file = {/home/kellen/Zotero/storage/FRV2CM6E/Kim et al. - 2024 - Unsupervised-to-Online Reinforcement Learning.pdf},\n langid = {english},\n abstract = {Offline-to-online reinforcement learning (RL), a framework that trains a policy with offline RL and then further fine-tunes it with online RL, has been considered a promising recipe for data-driven decision-making. While sensible, this framework has drawbacks: it requires domain-specific offline RL pre-training for each task, and is often brittle in practice. In this work, we propose unsupervised-to-online RL (U2O RL), which replaces domain-specific supervised offline RL with unsupervised offline RL, as a better alternative to offline-to-online RL. U2O RL not only enables reusing a single pre-trained model for multiple downstream tasks, but also learns better representations, which often result in even better performance and stability than supervised offline-to-online RL. To instantiate U2O RL in practice, we propose a general recipe for U2O RL to bridge task-agnostic unsupervised offline skill-based policy pre-training and supervised online fine-tuning. Throughout our experiments in nine state-based and pixel-based environments, we empirically demonstrate that U2O RL achieves strong performance that matches or even outperforms previous offline-to-online RL approaches, while being able to reuse a single pre-trained model for a number of different downstream tasks.},\n month = {August}\n}\n"}]