[{"abstract": "In continual learning, a learner has to keep learning from the data over its whole life time. A key issue is to decide what knowledge to keep and what knowledge to let go. In a neural network, this can be implemented by using a step-size vector to scale how much gradient samples change network weights. Common algorithms, like RMSProp and Adam, use heuristics, specifically normalization, to adapt this step-size vector. In this paper, we show that those heuristics ignore the effect of their adaptation on the overall objective function, for example by moving the step-size vector away from better step-size vectors. On the other hand, stochastic meta-gradient descent algorithms, like IDBD (Sutton, 1992), explicitly optimize the step-size vector with respect to the overall objective function. On simple problems, we show that IDBD is able to consistently improve step-size vectors, where RMSProp and Adam do not. We explain the differences between the two approaches and their respective limitations. We conclude by suggesting that combining both approaches could be a promising future direction to improve the performance of neural networks in continual learning.", "accessed": {"date-parts": [[2024, 9, 6]]}, "author": [{"family": "Degris", "given": "Thomas"}, {"family": "Javed", "given": "Khurram"}, {"family": "Sharifnassab", "given": "Arsalan"}, {"family": "Liu", "given": "Yuxin"}, {"family": "Sutton", "given": "Richard"}], "container-title": "arXiv.org", "id": "degrisStepsizeOptimizationContinual2024", "issued": {"date-parts": [[2024, 1]]}, "language": "en-US", "publisher": "https://arxiv.org/abs/2401.17401v1", "title": "Step-size Optimization for Continual Learning", "type": "", "original_bibtex": "@misc{degrisStepsizeOptimizationContinual2024,\n title = {Step-Size {{Optimization}} for {{Continual Learning}}},\n author = {Degris, Thomas and Javed, Khurram and Sharifnassab, Arsalan and Liu, Yuxin and Sutton, Richard},\n year = {2024},\n urldate = {2024-09-06},\n howpublished = {https://arxiv.org/abs/2401.17401v1},\n journal = {arXiv.org},\n file = {/home/kellen/Zotero/storage/YSGENB8H/Degris et al. - 2024 - Step-size Optimization for Continual Learning.pdf},\n langid = {english},\n abstract = {In continual learning, a learner has to keep learning from the data over its whole life time. A key issue is to decide what knowledge to keep and what knowledge to let go. In a neural network, this can be implemented by using a step-size vector to scale how much gradient samples change network weights. Common algorithms, like RMSProp and Adam, use heuristics, specifically normalization, to adapt this step-size vector. In this paper, we show that those heuristics ignore the effect of their adaptation on the overall objective function, for example by moving the step-size vector away from better step-size vectors. On the other hand, stochastic meta-gradient descent algorithms, like IDBD (Sutton, 1992), explicitly optimize the step-size vector with respect to the overall objective function. On simple problems, we show that IDBD is able to consistently improve step-size vectors, where RMSProp and Adam do not. We explain the differences between the two approaches and their respective limitations. We conclude by suggesting that combining both approaches could be a promising future direction to improve the performance of neural networks in continual learning.},\n month = {January}\n}\n"}]