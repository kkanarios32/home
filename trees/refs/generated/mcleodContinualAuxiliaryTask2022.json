[{"URL": "https://arxiv.org/abs/2202.11133", "abstract": "Learning auxiliary tasks, such as multiple predictions about the world, can provide many benefits to reinforcement learning systems. A variety of off-policy learning algorithms have been developed to learn such predictions, but as yet there is little work on how to adapt the behavior to gather useful data for those off-policy predictions. In this work, we investigate a reinforcement learning system designed to learn a collection of auxiliary tasks, with a behavior policy learning to take actions to improve those auxiliary predictions. We highlight the inherent non-stationarity in this continual auxiliary task learning problem, for both prediction learners and the behavior learner. We develop an algorithm based on successor features that facilitates tracking under non-stationary rewards, and prove the separation into learning successor features and rewards provides convergence rate improvements. We conduct an in-depth study into the resulting multi-prediction learning system.", "accessed": {"date-parts": [[2024, 10, 16]]}, "author": [{"family": "McLeod", "given": "Matthew"}, {"family": "Lo", "given": "Chunlok"}, {"family": "Schlegel", "given": "Matthew"}, {"family": "Jacobsen", "given": "Andrew"}, {"family": "Kumaraswamy", "given": "Raksha"}, {"family": "White", "given": "Martha"}, {"family": "White", "given": "Adam"}], "id": "mcleodContinualAuxiliaryTask2022", "issued": {"date-parts": [[2022, 2]]}, "keyword": "Computer Science - Machine Learning", "language": "en-US", "number": "arXiv:2202.11133", "publisher": "arXiv", "title": "Continual Auxiliary Task Learning", "type": "", "original_bibtex": "@misc{mcleodContinualAuxiliaryTask2022,\n title = {Continual {{Auxiliary Task Learning}}},\n author = {McLeod, Matthew and Lo, Chunlok and Schlegel, Matthew and Jacobsen, Andrew and Kumaraswamy, Raksha and White, Martha and White, Adam},\n year = {2022},\n urldate = {2024-10-16},\n number = {arXiv:2202.11133},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/GUALHQEZ/McLeod et al. - 2022 - Continual Auxiliary Task Learning.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Learning auxiliary tasks, such as multiple predictions about the world, can provide many benefits to reinforcement learning systems. A variety of off-policy learning algorithms have been developed to learn such predictions, but as yet there is little work on how to adapt the behavior to gather useful data for those off-policy predictions. In this work, we investigate a reinforcement learning system designed to learn a collection of auxiliary tasks, with a behavior policy learning to take actions to improve those auxiliary predictions. We highlight the inherent non-stationarity in this continual auxiliary task learning problem, for both prediction learners and the behavior learner. We develop an algorithm based on successor features that facilitates tracking under non-stationary rewards, and prove the separation into learning successor features and rewards provides convergence rate improvements. We conduct an in-depth study into the resulting multi-prediction learning system.},\n primaryclass = {cs},\n eprint = {2202.11133},\n month = {February}\n}\n"}]