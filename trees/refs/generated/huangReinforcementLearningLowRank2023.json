[{"URL": "https://arxiv.org/abs/2302.02252", "abstract": "MDPs with low-rank transitions\u2014that is, the transition matrix can be factored into the product of two matrices, left and right\u2014is a highly representative structure that enables tractable learning. The left matrix enables expressive function approximation for value-based learning and has been studied extensively. In this work, we instead investigate sample-efficient learning with density features, i.e., the right matrix, which induce powerful models for state-occupancy distributions. This setting not only sheds light on leveraging unsupervised learning in RL, but also enables plug-in solutions for convex RL. In the offline setting, we propose an algorithm for off-policy estimation of occupancies that can handle non-exploratory data. Using this as a subroutine, we further devise an online algorithm that constructs exploratory data distributions in a level-by-level manner. As a central technical challenge, the additive error of occupancy estimation is incompatible with the multiplicative definition of data coverage. In the absence of strong assumptions like reachability, this incompatibility easily leads to exponential error blow-up, which we overcome via novel technical tools. Our results also readily extend to the representation learning setting, when the density features are unknown and must be learned from an exponentially large candidate set.", "accessed": {"date-parts": [[2024, 9, 16]]}, "author": [{"family": "Huang", "given": "Audrey"}, {"family": "Chen", "given": "Jinglin"}, {"family": "Jiang", "given": "Nan"}], "id": "huangReinforcementLearningLowRank2023", "issued": {"date-parts": [[2023, 2]]}, "keyword": "Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning", "language": "en-US", "number": "arXiv:2302.02252", "publisher": "arXiv", "title": "Reinforcement Learning in Low-Rank MDPs with Density Features", "type": "", "original_bibtex": "@misc{huangReinforcementLearningLowRank2023,\n title = {Reinforcement {{Learning}} in {{Low-Rank MDPs}} with {{Density Features}}},\n author = {Huang, Audrey and Chen, Jinglin and Jiang, Nan},\n year = {2023},\n urldate = {2024-09-16},\n number = {arXiv:2302.02252},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/JV6TZ7S9/Huang et al. - 2023 - Reinforcement Learning in Low-Rank MDPs with Density Features.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {MDPs with low-rank transitions---that is, the transition matrix can be factored into the product of two matrices, left and right---is a highly representative structure that enables tractable learning. The left matrix enables expressive function approximation for value-based learning and has been studied extensively. In this work, we instead investigate sample-efficient learning with density features, i.e., the right matrix, which induce powerful models for state-occupancy distributions. This setting not only sheds light on leveraging unsupervised learning in RL, but also enables plug-in solutions for convex RL. In the offline setting, we propose an algorithm for off-policy estimation of occupancies that can handle non-exploratory data. Using this as a subroutine, we further devise an online algorithm that constructs exploratory data distributions in a level-by-level manner. As a central technical challenge, the additive error of occupancy estimation is incompatible with the multiplicative definition of data coverage. In the absence of strong assumptions like reachability, this incompatibility easily leads to exponential error blow-up, which we overcome via novel technical tools. Our results also readily extend to the representation learning setting, when the density features are unknown and must be learned from an exponentially large candidate set.},\n primaryclass = {cs, stat},\n eprint = {2302.02252},\n month = {February}\n}\n"}]