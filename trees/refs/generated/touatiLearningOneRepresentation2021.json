[{"DOI": "10.48550/arXiv.2103.07945", "URL": "https://arxiv.org/abs/2103.07945", "abstract": "We introduce the forward-backward (FB) representation of the dynamics of a reward-free Markov decision process. It provides explicit near-optimal policies for any reward specified a posteriori. During an unsupervised phase, we use reward-free interactions with the environment to learn two representations via off-the-shelf deep learning methods and temporal difference (TD) learning. In the test phase, a reward representation is estimated either from observations or an explicit reward description (e.g., a target state). The optimal policy for that reward is directly obtained from these representations, with no planning. We assume access to an exploration scheme or replay buffer for the first phase. The corresponding unsupervised loss is well-principled: if training is perfect, the policies obtained are provably optimal for any reward function. With imperfect training, the sub-optimality is proportional to the unsupervised approximation error. The FB representation learns long-range relationships between states and actions, via a predictive occupancy map, without having to synthesize states as in model-based approaches. This is a step towards learning controllable agents in arbitrary black-box stochastic environments. This approach compares well to goal-oriented RL algorithms on discrete and continuous mazes, pixel-based MsPacman, and the FetchReach virtual robot arm. We also illustrate how the agent can immediately adapt to new tasks beyond goal-oriented RL.", "accessed": {"date-parts": [[2024, 9, 18]]}, "author": [{"family": "Touati", "given": "Ahmed"}, {"family": "Ollivier", "given": "Yann"}], "id": "touatiLearningOneRepresentation2021", "issued": {"date-parts": [[2021, 10]]}, "keyword": "Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control", "number": "arXiv:2103.07945", "publisher": "arXiv", "title": "Learning One Representation to Optimize All Rewards", "type": "", "original_bibtex": "@misc{touatiLearningOneRepresentation2021,\n title = {Learning {{One Representation}} to {{Optimize All Rewards}}},\n author = {Touati, Ahmed and Ollivier, Yann},\n year = {2021},\n doi = {10.48550/arXiv.2103.07945},\n urldate = {2024-09-18},\n number = {arXiv:2103.07945},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/FN8MPGES/Touati and Ollivier - 2021 - Learning One Representation to Optimize All Rewards.pdf;/home/kellen/Zotero/storage/SXVNRKWC/2103.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control},\n archiveprefix = {arXiv},\n abstract = {We introduce the forward-backward (FB) representation of the dynamics of a reward-free Markov decision process. It provides explicit near-optimal policies for any reward specified a posteriori. During an unsupervised phase, we use reward-free interactions with the environment to learn two representations via off-the-shelf deep learning methods and temporal difference (TD) learning. In the test phase, a reward representation is estimated either from observations or an explicit reward description (e.g., a target state). The optimal policy for that reward is directly obtained from these representations, with no planning. We assume access to an exploration scheme or replay buffer for the first phase. The corresponding unsupervised loss is well-principled: if training is perfect, the policies obtained are provably optimal for any reward function. With imperfect training, the sub-optimality is proportional to the unsupervised approximation error. The FB representation learns long-range relationships between states and actions, via a predictive occupancy map, without having to synthesize states as in model-based approaches. This is a step towards learning controllable agents in arbitrary black-box stochastic environments. This approach compares well to goal-oriented RL algorithms on discrete and continuous mazes, pixel-based MsPacman, and the FetchReach virtual robot arm. We also illustrate how the agent can immediately adapt to new tasks beyond goal-oriented RL.},\n primaryclass = {cs, math},\n eprint = {2103.07945},\n month = {October}\n}\n"}]