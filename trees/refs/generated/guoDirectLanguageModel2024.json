[{"URL": "https://arxiv.org/abs/2402.04792", "abstract": "Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods. We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator.", "accessed": {"date-parts": [[2024, 9, 11]]}, "author": [{"family": "Guo", "given": "Shangmin"}, {"family": "Zhang", "given": "Biao"}, {"family": "Liu", "given": "Tianlin"}, {"family": "Liu", "given": "Tianqi"}, {"family": "Khalman", "given": "Misha"}, {"family": "Llinares", "given": "Felipe"}, {"family": "Rame", "given": "Alexandre"}, {"family": "Mesnard", "given": "Thomas"}, {"family": "Zhao", "given": "Yao"}, {"family": "Piot", "given": "Bilal"}, {"family": "Ferret", "given": "Johan"}, {"family": "Blondel", "given": "Mathieu"}], "id": "guoDirectLanguageModel2024", "issued": {"date-parts": [[2024, 2]]}, "keyword": "Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction", "language": "en-US", "number": "arXiv:2402.04792", "publisher": "arXiv", "title": "Direct Language Model Alignment from Online AI Feedback", "type": "", "original_bibtex": "@misc{guoDirectLanguageModel2024,\n title = {Direct {{Language Model Alignment}} from {{Online AI Feedback}}},\n author = {Guo, Shangmin and Zhang, Biao and Liu, Tianlin and Liu, Tianqi and Khalman, Misha and Llinares, Felipe and Rame, Alexandre and Mesnard, Thomas and Zhao, Yao and Piot, Bilal and Ferret, Johan and Blondel, Mathieu},\n year = {2024},\n urldate = {2024-09-11},\n number = {arXiv:2402.04792},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/GTI664DM/Guo et al. - 2024 - Direct Language Model Alignment from Online AI Feedback.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods. We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator.},\n primaryclass = {cs},\n eprint = {2402.04792},\n month = {February}\n}\n"}]