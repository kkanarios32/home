[{"URL": "https://arxiv.org/abs/2406.05883", "abstract": "Policy alignment of large language models refers to constrained policy optimization, where the policy is optimized to maximize a reward while staying close to a reference policy with respect to an $f$-divergence such as the $\\mathsf{KL}$ divergence. The best of $n$ alignment policy selects a sample from the reference policy that has the maximum reward among $n$ independent samples. For both cases (policy alignment and best of $n$), recent works showed empirically that the reward improvement of the aligned policy on the reference one scales like $\\sqrt{\\mathsf{KL}}$, with an explicit bound in $n$ on the $\\mathsf{KL}$ for the best of $n$ policy. We show in this paper that the $\\sqrt{\\mathsf{KL}}$ information theoretic upper bound holds if the reward under the reference policy has sub-gaussian tails. Moreover, we prove for the best of $n$ policy, that the $\\mathsf{KL}$ upper bound can be obtained for any $f$-divergence via a reduction to exponential order statistics owing to the R\\\u2019enyi representation of order statistics, and a data processing inequality. If additional information is known on the tails of the aligned policy we show that tighter control on the reward improvement can be obtained via the R\\\u2019enyi divergence. Finally we demonstrate how these upper bounds transfer from proxy rewards to golden rewards which results in a decrease in the golden reward improvement due to overestimation and approximation errors of the proxy reward.", "accessed": {"date-parts": [[2024, 9, 18]]}, "author": [{"family": "Mroueh", "given": "Youssef"}], "id": "mrouehInformationTheoreticGuarantees2024", "issued": {"date-parts": [[2024, 6]]}, "keyword": "Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning", "language": "en-US", "number": "arXiv:2406.05883", "publisher": "arXiv", "title": "Information Theoretic Guarantees For Policy Alignment In Large Language Models", "type": "", "original_bibtex": "@misc{mrouehInformationTheoreticGuarantees2024,\n title = {Information {{Theoretic Guarantees For Policy Alignment In Large Language Models}}},\n author = {Mroueh, Youssef},\n year = {2024},\n urldate = {2024-09-18},\n number = {arXiv:2406.05883},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/5NKQB3ZU/Mroueh - 2024 - Information Theoretic Guarantees For Policy Alignment In Large Language Models.pdf},\n keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Policy alignment of large language models refers to constrained policy optimization, where the policy is optimized to maximize a reward while staying close to a reference policy with respect to an \\$f\\$-divergence such as the \\${\\textbackslash}mathsf\\{KL\\}\\$ divergence. The best of \\$n\\$ alignment policy selects a sample from the reference policy that has the maximum reward among \\$n\\$ independent samples. For both cases (policy alignment and best of \\$n\\$), recent works showed empirically that the reward improvement of the aligned policy on the reference one scales like \\${\\textbackslash}sqrt\\{{\\textbackslash}mathsf\\{KL\\}\\}\\$, with an explicit bound in \\$n\\$ on the \\${\\textbackslash}mathsf\\{KL\\}\\$ for the best of \\$n\\$ policy. We show in this paper that the \\${\\textbackslash}sqrt\\{{\\textbackslash}mathsf\\{KL\\}\\}\\$ information theoretic upper bound holds if the reward under the reference policy has sub-gaussian tails. Moreover, we prove for the best of \\$n\\$ policy, that the \\${\\textbackslash}mathsf\\{KL\\}\\$ upper bound can be obtained for any \\$f\\$-divergence via a reduction to exponential order statistics owing to the R{\\textbackslash}'enyi representation of order statistics, and a data processing inequality. If additional information is known on the tails of the aligned policy we show that tighter control on the reward improvement can be obtained via the R{\\textbackslash}'enyi divergence. Finally we demonstrate how these upper bounds transfer from proxy rewards to golden rewards which results in a decrease in the golden reward improvement due to overestimation and approximation errors of the proxy reward.},\n primaryclass = {cs, math, stat},\n eprint = {2406.05883},\n month = {June}\n}\n"}]