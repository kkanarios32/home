[{"URL": "https://arxiv.org/abs/2405.16236", "abstract": "Modern large language model (LLM) alignment techniques rely on human feedback, but it is unclear whether the techniques fundamentally limit the capabilities of aligned LLMs. In particular, it is unclear whether it is possible to align (stronger) LLMs with superhuman capabilities with (weaker) human feedback without degrading their capabilities. This is an instance of the weak-to-strong generalization problem: using weaker (less capable) feedback to train a stronger (more capable) model. We prove that weak-to-strong generalization is possible by eliciting latent knowledge from pre-trained LLMs. In particular, we cast the weak-to-strong generalization problem as a transfer learning problem in which we wish to transfer a latent concept from a weak model to a strong pre-trained model. We prove that a naive fine-tuning approach suffers from fundamental limitations, but an alternative refinement-based approach suggested by the problem structure provably overcomes the limitations of fine-tuning. Finally, we demonstrate the practical applicability of the refinement approach in three LLM alignment tasks.", "accessed": {"date-parts": [[2024, 9, 25]]}, "author": [{"family": "Somerstep", "given": "Seamus"}, {"family": "Polo", "given": "Felipe Maia"}, {"family": "Banerjee", "given": "Moulinath"}, {"family": "Ritov", "given": "Ya\u2019acov"}, {"family": "Yurochkin", "given": "Mikhail"}, {"family": "Sun", "given": "Yuekai"}], "id": "somerstepStatisticalFrameworkWeakstrong2024", "issued": {"date-parts": [[2024, 5]]}, "keyword": "Computer Science - Machine Learning,Statistics - Machine Learning", "language": "en-US", "number": "arXiv:2405.16236", "publisher": "arXiv", "title": "A statistical framework for weak-to-strong generalization", "type": "", "original_bibtex": "@misc{somerstepStatisticalFrameworkWeakstrong2024,\n title = {A Statistical Framework for Weak-to-Strong Generalization},\n author = {Somerstep, Seamus and Polo, Felipe Maia and Banerjee, Moulinath and Ritov, Ya'acov and Yurochkin, Mikhail and Sun, Yuekai},\n year = {2024},\n urldate = {2024-09-25},\n number = {arXiv:2405.16236},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/UIG2LRFF/Somerstep et al. - 2024 - A statistical framework for weak-to-strong general.pdf},\n keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Modern large language model (LLM) alignment techniques rely on human feedback, but it is unclear whether the techniques fundamentally limit the capabilities of aligned LLMs. In particular, it is unclear whether it is possible to align (stronger) LLMs with superhuman capabilities with (weaker) human feedback without degrading their capabilities. This is an instance of the weak-to-strong generalization problem: using weaker (less capable) feedback to train a stronger (more capable) model. We prove that weak-to-strong generalization is possible by eliciting latent knowledge from pre-trained LLMs. In particular, we cast the weak-to-strong generalization problem as a transfer learning problem in which we wish to transfer a latent concept from a weak model to a strong pre-trained model. We prove that a naive fine-tuning approach suffers from fundamental limitations, but an alternative refinement-based approach suggested by the problem structure provably overcomes the limitations of fine-tuning. Finally, we demonstrate the practical applicability of the refinement approach in three LLM alignment tasks.},\n primaryclass = {cs, stat},\n eprint = {2405.16236},\n month = {May}\n}\n"}]