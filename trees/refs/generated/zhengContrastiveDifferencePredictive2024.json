[{"DOI": "10.48550/arXiv.2310.20141", "URL": "https://arxiv.org/abs/2310.20141", "abstract": "Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \\times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20 \\times$ more sample efficient than the successor representation and $1500 \\times$ more sample efficient than the standard (Monte Carlo) version of contrastive predictive coding.", "accessed": {"date-parts": [[2024, 9, 6]]}, "author": [{"family": "Zheng", "given": "Chongyi"}, {"family": "Salakhutdinov", "given": "Ruslan"}, {"family": "Eysenbach", "given": "Benjamin"}], "id": "zhengContrastiveDifferencePredictive2024", "issued": {"date-parts": [[2024, 2]]}, "keyword": "Computer Science - Artificial Intelligence,Computer Science - Machine Learning", "number": "arXiv:2310.20141", "publisher": "arXiv", "title": "Contrastive Difference Predictive Coding", "type": "", "original_bibtex": "@misc{zhengContrastiveDifferencePredictive2024,\n title = {Contrastive {{Difference Predictive Coding}}},\n author = {Zheng, Chongyi and Salakhutdinov, Ruslan and Eysenbach, Benjamin},\n year = {2024},\n doi = {10.48550/arXiv.2310.20141},\n urldate = {2024-09-06},\n number = {arXiv:2310.20141},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/LXYBRXB5/Zheng et al. - 2024 - Contrastive Difference Predictive Coding.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves \\$2 {\\textbackslash}times\\$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about \\$20 {\\textbackslash}times\\$ more sample efficient than the successor representation and \\$1500 {\\textbackslash}times\\$ more sample efficient than the standard (Monte Carlo) version of contrastive predictive coding.},\n primaryclass = {cs},\n eprint = {2310.20141},\n month = {February}\n}\n"}]