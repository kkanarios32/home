[{"URL": "https://arxiv.org/abs/2005.01643", "abstract": "In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.", "accessed": {"date-parts": [[2024, 11, 17]]}, "author": [{"family": "Levine", "given": "Sergey"}, {"family": "Kumar", "given": "Aviral"}, {"family": "Tucker", "given": "George"}, {"family": "Fu", "given": "Justin"}], "id": "levineOfflineReinforcementLearning2020", "issued": {"date-parts": [[2020, 11]]}, "keyword": "Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning", "language": "en-US", "number": "arXiv:2005.01643", "publisher": "arXiv", "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems", "title-short": "Offline Reinforcement Learning", "type": "", "original_bibtex": "@misc{levineOfflineReinforcementLearning2020,\n title = {Offline {{Reinforcement Learning}}: {{Tutorial}}, {{Review}}, and {{Perspectives}} on {{Open Problems}}},\n author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},\n year = {2020},\n urldate = {2024-11-17},\n number = {arXiv:2005.01643},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/QV4MPWZ3/Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, and Perspectives on Open Problems.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},\n primaryclass = {cs},\n eprint = {2005.01643},\n month = {November},\n shorttitle = {Offline {{Reinforcement Learning}}}\n}\n"}]