[{"URL": "https://arxiv.org/abs/2405.16043", "abstract": "Strong student models can learn from weaker teachers: when trained on the predictions of a weaker model, a strong pretrained student can learn to correct the weak model\u2019s errors and generalize to examples where the teacher is not confident, even when these examples are excluded from training. This enables learning from cheap, incomplete, and possibly incorrect label information, such as coarse logical rules or the generations of a language model. We show that existing weak supervision theory fails to account for both of these effects, which we call pseudolabel correction and coverage expansion, respectively. We give a new bound based on expansion properties of the data distribution and student hypothesis class that directly accounts for pseudolabel correction and coverage expansion. Our bounds capture the intuition that weak-to-strong generalization occurs when the strong model is unable to fit the mistakes of the weak teacher without incurring additional error. We show that these expansion properties can be checked from finite data and give empirical evidence that they hold in practice.", "accessed": {"date-parts": [[2024, 9, 18]]}, "author": [{"family": "Lang", "given": "Hunter"}, {"family": "Sontag", "given": "David"}, {"family": "Vijayaraghavan", "given": "Aravindan"}], "id": "langTheoreticalAnalysisWeakStrong2024", "issued": {"date-parts": [[2024, 5]]}, "keyword": "Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning", "language": "en-US", "number": "arXiv:2405.16043", "publisher": "arXiv", "title": "Theoretical Analysis of Weak-to-Strong Generalization", "type": "", "original_bibtex": "@misc{langTheoreticalAnalysisWeakStrong2024,\n title = {Theoretical {{Analysis}} of {{Weak-to-Strong Generalization}}},\n author = {Lang, Hunter and Sontag, David and Vijayaraghavan, Aravindan},\n year = {2024},\n urldate = {2024-09-18},\n number = {arXiv:2405.16043},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/LJQ96YTP/Lang et al. - 2024 - Theoretical Analysis of Weak-to-Strong Generalization.pdf},\n keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Strong student models can learn from weaker teachers: when trained on the predictions of a weaker model, a strong pretrained student can learn to correct the weak model's errors and generalize to examples where the teacher is not confident, even when these examples are excluded from training. This enables learning from cheap, incomplete, and possibly incorrect label information, such as coarse logical rules or the generations of a language model. We show that existing weak supervision theory fails to account for both of these effects, which we call pseudolabel correction and coverage expansion, respectively. We give a new bound based on expansion properties of the data distribution and student hypothesis class that directly accounts for pseudolabel correction and coverage expansion. Our bounds capture the intuition that weak-to-strong generalization occurs when the strong model is unable to fit the mistakes of the weak teacher without incurring additional error. We show that these expansion properties can be checked from finite data and give empirical evidence that they hold in practice.},\n primaryclass = {cs, stat},\n eprint = {2405.16043},\n month = {May}\n}\n"}]