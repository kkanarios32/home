[{"DOI": "10.48550/arXiv.2411.19418", "URL": "https://arxiv.org/abs/2411.19418", "abstract": "Having explored an environment, intelligent agents should be able to transfer their knowledge to most downstream tasks within that environment. Referred to as \"zero-shot learning,\" this ability remains elusive for general-purpose reinforcement learning algorithms. While recent works have attempted to produce zero-shot RL agents, they make assumptions about the nature of the tasks or the structure of the MDP. We present \\emph{Proto Successor Measure}: the basis set for all possible solutions of Reinforcement Learning in a dynamical system. We provably show that any possible policy can be represented using an affine combination of these policy independent basis functions. Given a reward function at test time, we simply need to find the right set of linear weights to combine these basis corresponding to the optimal policy. We derive a practical algorithm to learn these basis functions using only interaction data from the environment and show that our approach can produce the optimal policy at test time for any given reward function without additional environmental interactions. Project page: https://agarwalsiddhant10.github.io/projects/psm.html.", "accessed": {"date-parts": [[2024, 12, 4]]}, "author": [{"family": "Agarwal", "given": "Siddhant"}, {"family": "Sikchi", "given": "Harshit"}, {"family": "Stone", "given": "Peter"}, {"family": "Zhang", "given": "Amy"}], "id": "agarwalProtoSuccessorMeasure2024", "issued": {"date-parts": [[2024, 11]]}, "keyword": "Computer Science - Artificial Intelligence,Computer Science - Machine Learning", "language": "en-US", "number": "arXiv:2411.19418", "publisher": "arXiv", "title": "Proto Successor Measure: Representing the Space of All Possible Solutions of Reinforcement Learning", "title-short": "Proto Successor Measure", "type": "", "original_bibtex": "@misc{agarwalProtoSuccessorMeasure2024,\n title = {Proto {{Successor Measure}}: {{Representing}} the {{Space}} of {{All Possible Solutions}} of {{Reinforcement Learning}}},\n author = {Agarwal, Siddhant and Sikchi, Harshit and Stone, Peter and Zhang, Amy},\n year = {2024},\n doi = {10.48550/arXiv.2411.19418},\n urldate = {2024-12-04},\n number = {arXiv:2411.19418},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/IZN7I7NS/Agarwal et al. - 2024 - Proto Successor Measure Representing the Space of All Possible Solutions of Reinforcement Learning.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Having explored an environment, intelligent agents should be able to transfer their knowledge to most downstream tasks within that environment. Referred to as \"zero-shot learning,\" this ability remains elusive for general-purpose reinforcement learning algorithms. While recent works have attempted to produce zero-shot RL agents, they make assumptions about the nature of the tasks or the structure of the MDP. We present {\\textbackslash}emph\\{Proto Successor Measure\\}: the basis set for all possible solutions of Reinforcement Learning in a dynamical system. We provably show that any possible policy can be represented using an affine combination of these policy independent basis functions. Given a reward function at test time, we simply need to find the right set of linear weights to combine these basis corresponding to the optimal policy. We derive a practical algorithm to learn these basis functions using only interaction data from the environment and show that our approach can produce the optimal policy at test time for any given reward function without additional environmental interactions. Project page: https://agarwalsiddhant10.github.io/projects/psm.html.},\n primaryclass = {cs},\n eprint = {2411.19418},\n month = {November},\n shorttitle = {Proto {{Successor Measure}}}\n}\n"}]