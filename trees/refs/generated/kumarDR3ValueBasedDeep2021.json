[{"URL": "https://arxiv.org/abs/2112.04716", "abstract": "Despite overparameterization, deep networks trained via supervised learning are easy to optimize and exhibit excellent generalization. One hypothesis to explain this is that overparameterized deep networks enjoy the benefits of implicit regularization induced by stochastic gradient descent, which favors parsimonious solutions that generalize well on test inputs. It is reasonable to surmise that deep reinforcement learning (RL) methods could also benefit from this effect. In this paper, we discuss how the implicit regularization effect of SGD seen in supervised learning could in fact be harmful in the offline deep RL setting, leading to poor generalization and degenerate feature representations. Our theoretical analysis shows that when existing models of implicit regularization are applied to temporal difference learning, the resulting derived regularizer favors degenerate solutions with excessive \u201caliasing\u201d, in stark contrast to the supervised learning case. We back up these findings empirically, showing that feature representations learned by a deep network value function trained via bootstrapping can indeed become degenerate, aliasing the representations for state-action pairs that appear on either side of the Bellman backup. To address this issue, we derive the form of this implicit regularizer and, inspired by this derivation, propose a simple and effective explicit regularizer, called DR3, that counteracts the undesirable effects of this implicit regularizer. When combined with existing offline RL methods, DR3 substantially improves performance and stability, alleviating unlearning in Atari 2600 games, D4RL domains and robotic manipulation from images.", "accessed": {"date-parts": [[2024, 9, 11]]}, "author": [{"family": "Kumar", "given": "Aviral"}, {"family": "Agarwal", "given": "Rishabh"}, {"family": "Ma", "given": "Tengyu"}, {"family": "Courville", "given": "Aaron"}, {"family": "Tucker", "given": "George"}, {"family": "Levine", "given": "Sergey"}], "id": "kumarDR3ValueBasedDeep2021", "issued": {"date-parts": [[2021, 12]]}, "keyword": "Computer Science - Machine Learning", "language": "en-US", "number": "arXiv:2112.04716", "publisher": "arXiv", "title": "DR3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization", "title-short": "DR3", "type": "", "original_bibtex": "@misc{kumarDR3ValueBasedDeep2021,\n title = {{{DR3}}: {{Value-Based Deep Reinforcement Learning Requires Explicit Regularization}}},\n author = {Kumar, Aviral and Agarwal, Rishabh and Ma, Tengyu and Courville, Aaron and Tucker, George and Levine, Sergey},\n year = {2021},\n urldate = {2024-09-11},\n number = {arXiv:2112.04716},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/S9MJBMRM/Kumar et al. - 2021 - DR3 Value-Based Deep Reinforcement Learning Requires Explicit Regularization.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Despite overparameterization, deep networks trained via supervised learning are easy to optimize and exhibit excellent generalization. One hypothesis to explain this is that overparameterized deep networks enjoy the benefits of implicit regularization induced by stochastic gradient descent, which favors parsimonious solutions that generalize well on test inputs. It is reasonable to surmise that deep reinforcement learning (RL) methods could also benefit from this effect. In this paper, we discuss how the implicit regularization effect of SGD seen in supervised learning could in fact be harmful in the offline deep RL setting, leading to poor generalization and degenerate feature representations. Our theoretical analysis shows that when existing models of implicit regularization are applied to temporal difference learning, the resulting derived regularizer favors degenerate solutions with excessive ``aliasing'', in stark contrast to the supervised learning case. We back up these findings empirically, showing that feature representations learned by a deep network value function trained via bootstrapping can indeed become degenerate, aliasing the representations for state-action pairs that appear on either side of the Bellman backup. To address this issue, we derive the form of this implicit regularizer and, inspired by this derivation, propose a simple and effective explicit regularizer, called DR3, that counteracts the undesirable effects of this implicit regularizer. When combined with existing offline RL methods, DR3 substantially improves performance and stability, alleviating unlearning in Atari 2600 games, D4RL domains and robotic manipulation from images.},\n primaryclass = {cs},\n eprint = {2112.04716},\n month = {December},\n shorttitle = {{{DR3}}}\n}\n"}]