[{"DOI": "10.48550/arXiv.2305.17333", "URL": "https://arxiv.org/abs/2305.17333", "abstract": "Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.", "accessed": {"date-parts": [[2024, 9, 8]]}, "author": [{"family": "Malladi", "given": "Sadhika"}, {"family": "Gao", "given": "Tianyu"}, {"family": "Nichani", "given": "Eshaan"}, {"family": "Damian", "given": "Alex"}, {"family": "Lee", "given": "Jason D."}, {"family": "Chen", "given": "Danqi"}, {"family": "Arora", "given": "Sanjeev"}], "id": "malladiFineTuningLanguageModels2024", "issued": {"date-parts": [[2024, 1]]}, "keyword": "Computer Science - Computation and Language,Computer Science - Machine Learning", "number": "arXiv:2305.17333", "publisher": "arXiv", "title": "Fine-Tuning Language Models with Just Forward Passes", "type": "", "original_bibtex": "@misc{malladiFineTuningLanguageModels2024,\n title = {Fine-{{Tuning Language Models}} with {{Just Forward Passes}}},\n author = {Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D. and Chen, Danqi and Arora, Sanjeev},\n year = {2024},\n doi = {10.48550/arXiv.2305.17333},\n urldate = {2024-09-08},\n number = {arXiv:2305.17333},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/2J3FEQX7/Malladi et al. - 2024 - Fine-Tuning Language Models with Just Forward Passes.pdf},\n keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.},\n primaryclass = {cs},\n eprint = {2305.17333},\n month = {January}\n}\n"}]