% ["references"]
\title{Continual Auxiliary Task Learning}
\date{2022-02}
\author{Matthew McLeod}\author{Chunlok Lo}\author{Matthew Schlegel}\author{Andrew Jacobsen}\author{Raksha Kumaraswamy}\author{Martha White}\author{Adam White}
\taxon{reference}
\meta{external}{https://arxiv.org/abs/2202.11133}

\meta{bibtex}{\startverb
@misc{mcleodContinualAuxiliaryTask2022,
 title = {Continual {{Auxiliary Task Learning}}},
 author = {McLeod, Matthew and Lo, Chunlok and Schlegel, Matthew and Jacobsen, Andrew and Kumaraswamy, Raksha and White, Martha and White, Adam},
 year = {2022},
 urldate = {2024-10-16},
 number = {arXiv:2202.11133},
 publisher = {arXiv},
 file = {/home/kellen/Zotero/storage/GUALHQEZ/McLeod et al. - 2022 - Continual Auxiliary Task Learning.pdf},
 keywords = {Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {Learning auxiliary tasks, such as multiple predictions about the world, can provide many benefits to reinforcement learning systems. A variety of off-policy learning algorithms have been developed to learn such predictions, but as yet there is little work on how to adapt the behavior to gather useful data for those off-policy predictions. In this work, we investigate a reinforcement learning system designed to learn a collection of auxiliary tasks, with a behavior policy learning to take actions to improve those auxiliary predictions. We highlight the inherent non-stationarity in this continual auxiliary task learning problem, for both prediction learners and the behavior learner. We develop an algorithm based on successor features that facilitates tracking under non-stationary rewards, and prove the separation into learning successor features and rewards provides convergence rate improvements. We conduct an in-depth study into the resulting multi-prediction learning system.},
 primaryclass = {cs},
 eprint = {2202.11133},
 month = {February}
}
\stopverb}