% ["references"]
\title{Information Theoretic Guarantees For Policy Alignment In Large Language Models}
\date{2024-06}
\author{Youssef Mroueh}
\taxon{reference}
\meta{external}{https://arxiv.org/abs/2406.05883}

\meta{bibtex}{\startverb
@misc{mrouehInformationTheoreticGuarantees2024,
 title = {Information {{Theoretic Guarantees For Policy Alignment In Large Language Models}}},
 author = {Mroueh, Youssef},
 year = {2024},
 urldate = {2024-09-18},
 number = {arXiv:2406.05883},
 publisher = {arXiv},
 file = {/home/kellen/Zotero/storage/5NKQB3ZU/Mroueh - 2024 - Information Theoretic Guarantees For Policy Alignment In Large Language Models.pdf},
 keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {Policy alignment of large language models refers to constrained policy optimization, where the policy is optimized to maximize a reward while staying close to a reference policy with respect to an \$f\$-divergence such as the \${\textbackslash}mathsf\{KL\}\$ divergence. The best of \$n\$ alignment policy selects a sample from the reference policy that has the maximum reward among \$n\$ independent samples. For both cases (policy alignment and best of \$n\$), recent works showed empirically that the reward improvement of the aligned policy on the reference one scales like \${\textbackslash}sqrt\{{\textbackslash}mathsf\{KL\}\}\$, with an explicit bound in \$n\$ on the \${\textbackslash}mathsf\{KL\}\$ for the best of \$n\$ policy. We show in this paper that the \${\textbackslash}sqrt\{{\textbackslash}mathsf\{KL\}\}\$ information theoretic upper bound holds if the reward under the reference policy has sub-gaussian tails. Moreover, we prove for the best of \$n\$ policy, that the \${\textbackslash}mathsf\{KL\}\$ upper bound can be obtained for any \$f\$-divergence via a reduction to exponential order statistics owing to the R{\textbackslash}'enyi representation of order statistics, and a data processing inequality. If additional information is known on the tails of the aligned policy we show that tighter control on the reward improvement can be obtained via the R{\textbackslash}'enyi divergence. Finally we demonstrate how these upper bounds transfer from proxy rewards to golden rewards which results in a decrease in the golden reward improvement due to overestimation and approximation errors of the proxy reward.},
 primaryclass = {cs, math, stat},
 eprint = {2406.05883},
 month = {June}
}
\stopverb}