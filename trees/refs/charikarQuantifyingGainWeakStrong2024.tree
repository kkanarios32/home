% ["references"]
\title{Quantifying the Gain in Weak-to-Strong Generalization}
\date{2024-10}
\author{Moses Charikar}\author{Chirag Pabbaraju}\author{Kirankumar Shiragur}
\taxon{reference}
\meta{external}{https://arxiv.org/abs/2405.15116}

\meta{bibtex}{\startverb
@misc{charikarQuantifyingGainWeakStrong2024,
 title = {Quantifying the {{Gain}} in {{Weak-to-Strong Generalization}}},
 author = {Charikar, Moses and Pabbaraju, Chirag and Shiragur, Kirankumar},
 year = {2024},
 urldate = {2024-10-24},
 number = {arXiv:2405.15116},
 publisher = {arXiv},
 file = {/home/kellen/Zotero/storage/Y3LYJ8Q8/Charikar et al. - 2024 - Quantifying the Gain in Weak-to-Strong Generalization.pdf},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {Recent advances in large language models have shown capabilities that are extraordinary and near-superhuman. These models operate with such complexity that reliably evaluating and aligning them proves challenging for humans. This leads to the natural question: can guidance from weak models (like humans) adequately direct the capabilities of strong models? In a recent and somewhat surprising work, Burns et al. [BIK+23] empirically demonstrated that when strong models (like GPT-4) are finetuned using labels generated by weak supervisors (like GPT-2), the strong models outperform their weaker counterparts---a phenomenon they term weak-to-strong generalization.},
 primaryclass = {cs},
 eprint = {2405.15116},
 month = {October}
}
\stopverb}