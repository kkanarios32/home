\date{2025-02-01}
\import{base-macros}
\title{Information Theory}
\author{kellenkanarios}
\tag{note}
\tag{unfinished}
\tag{lost}

\section{Lecture 5}{
\transclude{kak-004C}
\transclude{kak-004D}

\transclude{kak-0034}
\remark{
    This can be thought of as the tangent is always an underestimate of a convex function.
  }
\transclude{kak-0035}
\remark{
  This implies positive curvature or the common "bowl-shaped" interpretation.
  }
\transclude{kak-002G}
\remark{
    Holds with equality if and only if 
    \ol{
      \li{#{f} is linear on #{\mathcal{X}}}
      \li{#{X} is a constant almost surely.}
      }
  }
\transclude{kak-004E}
\problem{
  When is #{D_{\mathrm{KL}}(P || Q) = 0}?
}
\answer{
    Need #{Q(x) / P(x)} to be constant almost surely. This occurs only when #{Q = P}. 
    ##{
        Q(x) / P(x) = c \implies Q(x) = P(x) c \iff \sum Q(x) = c \sum P(x) \implies c = 1
      }
  }
  \strong{Applications of [[kak-004E]]:}
  \ul{
    \li{
      #{I(X; Y) \geq 0}
    }
    \li{#{H(X | Y) \leq H(X)}}
    \li{#{H(X^n) \leq \sum_{i = 1}^n H(X_i)}}
    \li{#{H(X) \leq \log |\mathcal{X}|}}
  }
  \transclude{kak-004F}
  \strong{Consequences of [[kak-004F]]:}
  \ul{
      \li{Joint convexity of #{D_{\mathrm{KL}}(P || Q)},}
      \li{Individual convexity of #{D_{\mathrm{KL}}(P || Q)}.}
      \li{Concavity of entropy.}
    }
}
\section{Lecture 7}{
\transclude{kak-004Y}
\section{Data Processing Inequality}{
    High-level
    \ul{
        \li{#{Y} contains more information about #{X} than #{\hat{X}}.}
        \li{We cannot increase the amount of information about #{X} by processing #{Y}.}
      }
    \strong{Markov Chains:}
    \ul{
        \li{#{(U, V, W)} form a Markov chain if
        ##{
\begin{align*}
  P(W = w \mid U = u, V = v) = P(w = w \mid V = v)
\end{align*}
          }
Denoted #{U \to V \to W}.
        }
          \li{Equivalently, we say #{(U \perp W) | V} which means ##{
             P(W = w, U=u \mid V = v) \implies P(W = w \mid V = v)P(U = u \mid V = v)
          }
          }
          \li{Reversibility: #{U \to V \to W \iff W \to V \to U}.}
          \li{Estimation: #{X \to Y \to \hat{X}}}
      } 

\transclude{kak-004Z}
}
\section{Fano's Inequality}{
    \ul{
        \li{#{H(X | Y)} controls the error probability #{P_e}.}
      }
\transclude{kak-0050}
}
  }
\section{Asymptotic Equipartion Propery}{
\transclude{kak-0051}
\transclude{kak-0052}
\transclude{kak-0053}
\transclude{kak-0054}
\transclude{kak-0055}
\p{
\strong{interpretation:} #{A_{\epsilon}^{(n)}\,\text{ is a subset of }\,{\mathcal{X}}^{n}}
\ul{
    \li{Contains almost all the probability.}
    \li{Consists of #{\approx 2^{nH(X)}}}
    \li{Almost equiprobable sequences.}
  }
}
\transclude{kak-0056}
}

\section{Fixed-rate lossless source coding}{
\transclude{kak-0059}
\p{
    \strong{Performance:}
    \ul{
        \li{Rate = #{\frac{\log_2 \theta}{n}}}
        \li{Probability of decoding error = #{\mathbb{P}|\hat{X}^n \neq X^n|}}
      }
  }
\transclude{kak-005A}
\transclude{kak-005B}
\transclude{kak-005C}
  }
