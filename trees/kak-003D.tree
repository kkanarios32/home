\date{2025-01-27}
\author{kellenkanarios}
\import{base-macros}
\tag{blog}
\tag{llms}
\title{Deepseek R1: RL is back!}

\p{In this blog, we will aim to understand the key contributions of [[deepseekai2025deepseekr1incentivizingreasoningcapability]]. Time permitting, we might go over the engineering innovations introduced in [[deepseekai2024deepseekv3technicalreport]].}

\section{How is R1 different then previous iterations of models?}{
  \ul{
    \li{In R1-Zero, they do \strong{ZERO} SFT on the base model - directly apply reinforcement learning.}
    \li{Use PPO like policy optimization but do \strong{NOT} learn a reward model.}
    \ul{
        \li{Use very simple reward: 
        \ul{
          \li{#{+1} for correct answer} 
          \li{#{-0.5} for incorrect answer} 
          \li{#{-1} for inability to answer.}
        }
        }
      }
  }
}

\transclude{kak-003X}

\section{Post-training}{
    \ul{
        \li{\em{Reinforcement Learning for all Scenarios:} Seems like they do RLHF after the pure RL stage.}
        \ul{
            \li{Do traditional helpfulness harmfulness RLHF with trained reward model.}
          }
      }
  }

\section{Distilling Models with R1}{
  \ul{
      \li{To distill, they do only SFT with R1 generated COT.}
      \li{They show that distillation outperforms doing pure RL approach on smaller model}
      \ul{
          \li{Seems contradictory to [[zeng2025simplerl]]}
        }
    }
  }
