\date{2025-01-27}
\author{kellenkanarios}
\import{base-macros}
\tag{blog}
\tag{llms}
\tag{upcoming}
\title{Deepseek v1 through R1: RL is back!}
\xmlns:html{http://www.w3.org/1999/xhtml}

\p{In this blog, we will aim to understand the key contributions of [[deepseekai2025deepseekr1incentivizingreasoningcapability]]. It will serve as the complement to my group meeting presentation possibly consisting of more in-depth explanations. Time permitting, we might go over the engineering innovations introduced in [[deepseekai2024deepseekv3technicalreport]].}
\section{Background}{
    By request of my advisor, I will cover the basics of LLMs prior to the innovations in the Deepseek lineage. For those familiar with LLMs, please skip this section.
    \transclude{kak-004J}
    \transclude{kak-004G}
    \section{RLHF}{
##{\mathrm{loss}\left(\phi\right)=E_{\left(x,y\right)\sim D_{\pi_{\phi}^{\mathrm{RL}}}}\left[r_\theta(x,y)-\beta\log\left(\pi_{\phi}^{\mathrm{RL}}(y\mid x)/\pi^{\mathrm{SFT}}(y\mid x)\right)\right] + \gamma E_{x\sim D_{\mathrm{pretrain}}}\left[\log(\pi_{\phi}^{\mathrm{RL}}(x))\right]}
      }
}

\section{Deepseek v2}{
    Paper 
  }

\section{Deepseek v3}{
TODO. Kinda wanna look into the architectural / training innovations from this paper.
  }

\section{Deepseek R1}{

\section{How is R1 different then previous iterations of models?}{
  \ul{
    \li{In R1-Zero, they do \strong{ZERO} SFT on the base model - directly apply reinforcement learning.}
    \li{Use PPO like policy optimization but do \strong{NOT} learn a reward model.}
    \ul{
        \li{Use very simple reward: 
        \ul{
          \li{#{+1} for correct answer} 
          \li{#{-0.5} for incorrect answer} 
          \li{#{-1} for inability to answer.}
        }
        }
      }
  }
}

\transclude{kak-003X}

\section{Post-training}{
    \ul{
        \li{\em{Reinforcement Learning for all Scenarios:} Seems like they do RLHF after the pure RL stage.}
        \ul{
            \li{Do traditional helpfulness harmfulness RLHF with trained reward model.}
          }
      }
  }

\section{Distilling Models with R1}{
  \ul{
      \li{To distill, they do only SFT with R1 generated COT.}
      \li{They show that distillation outperforms doing pure RL approach on smaller model}
      \ul{
          \li{Seems contradictory to [[zeng2025simplerl]]}
        }
    }
  }

}

\hr

\<html:script>[src]{https://utteranc.es/client.js}[repo]{kkanarios32/website-comments}[issue-term]{pathname}[theme]{boxy-light}[crossorigin]{anonymous}[async]{}{}
